\chapter{Benchmarking Results and Observations}
\label{chap:benchmarking}

This chapter presents a detailed empirical evaluation of the Keystone enclave framework, with a specific focus on its performance characteristics under a range of representative workloads. The primary aim is to quantify the computational and system-level overheads introduced by secure enclave execution on RISC-V platforms, and to explore how these overheads are influenced by underlying hardware configurations and architectural constraints. By systematically comparing enclave-based execution against native (non-isolated) execution, we aim to identify the key trade-offs and limitations that developers and system architects must consider when adopting enclaves for trusted execution.

To achieve this, we deploy a carefully selected set of benchmark workloads that span both general-purpose and security-critical application domains. These include the Dhrystone and CoreMark benchmarks—commonly used to assess CPU performance and embedded system efficiency—as well as the Kyber post-quantum cryptographic algorithm, which provides a real-world workload with stringent security and computational requirements. Together, these benchmarks provide a diverse and comprehensive view of how Keystone enclaves behave across different execution contexts.

The benchmarking process is structured to examine four interrelated dimensions. First, we establish a native performance baseline to isolate the impact of enclave-related operations. Second, we evaluate the performance of the same workloads when executed within enclaves, highlighting the latency and resource costs incurred by isolation mechanisms such as Physical Memory Protection (PMP) and context switching. Third, we perform a workload sensitivity analysis, comparing the behavior of different benchmark types—sequential vs. parallel, lightweight vs. compute-intensive—to determine how workload characteristics affect enclave performance. Finally, we assess the influence of key hardware parameters, including CPU core count, memory size, and cache configuration, on both native and enclave execution.

Through these experiments, we uncover both expected and non-trivial behaviors. For instance, while enclave overheads are generally modest—typically under 15\%—they are highly sensitive to parallelism and memory access patterns. Furthermore, we identify critical bottlenecks, such as the fixed number of PMP entries available for isolating memory regions, which directly limit enclave concurrency and scalability.

The results presented in this chapter serve as the empirical foundation for the interpretive analysis in the next chapter. By rigorously characterizing the performance profile of Keystone enclaves, we establish a data-driven basis for understanding the broader implications of enclave deployment in secure system design, as well as informing practical strategies for optimization and resource management.

\section{Baseline: Native (Non-Enclave) Performance}
\label{sec:baseline-native}

To accurately assess the performance overheads imposed by secure enclave execution, it is imperative to begin with a rigorously characterized baseline: one that isolates application-level behavior from security mechanisms, scheduling anomalies, and hardware acceleration artifacts. This section documents such a baseline, derived from native (non-enclave) execution of representative workloads on a deterministic, single-core RISC-V simulation environment. All benchmarks were executed under the Low-End Baseline configuration, designed to approximate the resource constraints of a lightweight embedded platform: 64~MB of main memory, a single in-order RISC-V core, and no parallelism beyond sequential instruction issue. This environment intentionally excludes vector extensions, out-of-order speculation, and memory prefetching, ensuring that measured performance reflects algorithmic structure rather than microarchitectural opportunism.

Each benchmark was executed both \emph{sequentially} and \emph{in parallel}, ten times each, and the resulting figures compare performance and stability across both modes.

% ------------------------
% Wall-clock time figures
% ------------------------
Figure~\ref{fig:wallclock_coremark} reports CoreMark's performance profile using the latest data from Run~10 of both sequential and parallel execution modes. Execution time remained consistent, with the sequential case averaging approximately 101.17 seconds of wall-clock time and the parallel case averaging around 103.37 seconds. This corresponds to an effective throughput of about 988.4 iterations per second for the sequential run and 967.4 iterations per second for the parallel run. User-mode CPU time dominated in both cases (101.20 seconds sequential, 103.39 seconds parallel), accounting for over 99.9\% of total CPU time. System-mode CPU time was negligible (under 0.03 seconds), confirming that the workload’s operational footprint is almost entirely in user space with minimal reliance on system calls or kernel transitions.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/wallclock_coremark.png}
%   \caption{CoreMark wall-clock time in native sequential and parallel runs.}
%   \label{fig:wallclock_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/wallclock_dhrystone.png}
%   \caption{Dhrystone wall-clock time in native sequential and parallel runs.}
%   \label{fig:wallclock_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/wallclock_kyber.png}
%   \caption{Kyber wall-clock time in native sequential and parallel runs.}
%   \label{fig:wallclock_kyber}
% \end{figure}

% ------------------------
% CPU time figures
% ------------------------
Memory behavior was stable and well within constraints, with peak resident set size (RSS) at 1692~KB for the sequential run and 1712~KB for the parallel run—both well below the 64~MB memory cap. This indicates that the benchmark’s data and code footprint remained fully cacheable, without triggering paging or dynamic memory expansion. Minor page faults were low and consistent (82--84 for sequential, 82--85 for parallel), though a single major page fault was observed in the parallel run and two in the sequential run, suggesting rare but possible demand paging during lazy allocation. Voluntary context switches were minimal (0--1 sequential, 0--3 parallel), while involuntary context switches showed a stark contrast: approximately 350 in the sequential case versus over 25,800 in the parallel case. This disparity reflects a significantly higher degree of preemption or timer interrupts in the parallel workload environment.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cputime_coremark.png}
%   \caption{CoreMark CPU time (user and system) in native sequential and parallel runs.}
%   \label{fig:cputime_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cputime_dhrystone.png}
%   \caption{Dhrystone CPU time (user and system) in native sequential and parallel runs.}
%   \label{fig:cputime_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cputime_kyber.png}
%   \caption{Kyber CPU time (user and system) in native sequential and parallel runs.}
%   \label{fig:cputime_kyber}
% \end{figure}

% ------------------------
% Throughput figures
% ------------------------
Figure~\ref{fig:dhrystone_native} presents detailed results from the Dhrystone benchmark under native conditions. The sequential runs exhibited an average elapsed time of approximately 101.2 seconds per run, with user-mode CPU time closely matching this figure, indicating near-total CPU utilization. In contrast, the parallel merged runs completed in roughly 103.4 seconds, with user CPU time again nearly equal to wall-clock time. Throughput, measured in Dhrystone operations per second (DPS), was higher in the sequential case—approximately 988.4~DPS versus 967.4~DPS in the parallel configuration—reflecting slight scheduling overhead from virtual concurrency under QEMU.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/throughput_coremark.png}
%   \caption{CoreMark throughput in native sequential and parallel runs.}
%   \label{fig:throughput_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/throughput_dhrystone.png}
%   \caption{Dhrystone throughput in native sequential and parallel runs.}
%   \label{fig:throughput_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/throughput_kyber.png}
%   \caption{Kyber throughput in native sequential and parallel runs.}
%   \label{fig:throughput_kyber}
% \end{figure}

% ------------------------
% Memory usage figures
% ------------------------
Memory footprint remained small (peak RSS ~1.7~MB) and stable, with 82--86 minor page faults per run.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/memory_rss_coremark.png}
%   \caption{CoreMark resident set size (RSS) in native sequential and parallel runs.}
%   \label{fig:memory_rss_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/memory_rss_dhrystone.png}
%   \caption{Dhrystone resident set size (RSS) in native sequential and parallel runs.}
%   \label{fig:memory_rss_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/memory_rss_kyber.png}
%   \caption{Kyber resident set size (RSS) in native sequential and parallel runs.}
%   \label{fig:memory_rss_kyber}
% \end{figure}

% ------------------------
% Page faults figures
% ------------------------
% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/pagefaults_coremark.png}
%   \caption{CoreMark minor and major page faults in native sequential and parallel runs.}
%   \label{fig:pagefaults_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/pagefaults_dhrystone.png}
%   \caption{Dhrystone minor and major page faults in native sequential and parallel runs.}
%   \label{fig:pagefaults_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/pagefaults_kyber.png}
%   \caption{Kyber minor and major page faults in native sequential and parallel runs.}
%   \label{fig:pagefaults_kyber}
% \end{figure}

% ------------------------
% Context switches figures
% ------------------------
Voluntary context switches were minimal (0--1 sequential, 0--3 parallel), while involuntary context switches showed a stark contrast: approximately 350 in the sequential case versus over 25,800 in the parallel case.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/contextswitches_coremark.png}
%   \caption{CoreMark voluntary and involuntary context switches in native sequential and parallel runs.}
%   \label{fig:contextswitches_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/contextswitches_dhrystone.png}
%   \caption{Dhrystone voluntary and involuntary context switches in native sequential and parallel runs.}
%   \label{fig:contextswitches_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/contextswitches_kyber.png}
%   \caption{Kyber voluntary and involuntary context switches in native sequential and parallel runs.}
%   \label{fig:contextswitches_kyber}
% \end{figure}

% ------------------------
% Cycle counts figures (Kyber only)
% ------------------------
Kyber performance was evaluated using the official \texttt{test\_speed512} binary from the reference implementation's \texttt{ref/} directory.\footnote{See \url{https://github.com/pq-crystals/kyber}. The \texttt{test\_speed512} utility executes keypair generation, encapsulation, and decapsulation 1000 times each and reports cycle counts using architecture-level counters emulated under QEMU.} Table~\ref{tab:kyber_run10_cycles} summarizes the median cycle counts for each operation, while Figure~\ref{fig:kyber_native_cycles} compares sequential and parallel execution.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cyclecounts_kyber.png}
%   \caption{Kyber median cycle counts for keypair generation, encapsulation, and decapsulation in native sequential and parallel runs.}
%   \label{fig:kyber_native_cycles}
% \end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/kyber_64M_1Core.pdf}
    \caption{Comparison of Kyber Median CPU Cycles per Operation: Sequential Single-Threaded (\texttt{run10\_SEQ}) vs Parallel Merged (\texttt{run10}) Execution (10 Runs)}
    \label{fig:kyber_native_cycles}
\end{figure}
All cryptographic operations executed entirely in user space, without invoking privileged instructions or incurring system-level overhead. Memory footprint remained minimal (peak RSS ~10~KB), with only a single minor page fault observed across all runs.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{|l|r|r|p{4.5cm}|}
\hline
\textbf{Operation} & \textbf{Parallel Run10 Median (cycles)} & \textbf{Sequential Run10 Median (cycles)} & \textbf{Rationale / Notes} \\
\hline
indcpa\_keypair & 1,870,200 & 1,702,800 & Baseline IND-CPA key generation cost for setting up public/private pairs. \\
\hline
indcpa\_enc     & 38,376,800 & 2,174,400 & Parallel vs sequential encryption overhead comparison. \\
\hline
indcpa\_dec     & 39,060,800 & 618,400   & Decryption efficiency gap between modes. \\
\hline
kyber\_keypair  & 1,870,200 & 1,800,300 & Full Kyber keypair generation timing. \\
\hline
kyber\_encaps   & 38,376,800 & 2,285,900 & Encapsulation performance with shared secret derivation. \\
\hline
kyber\_decaps   & 39,060,800 & 2,921,800 & Decapsulation latency and handshake completion time. \\
\hline
\end{tabular}
\caption{Median CPU cycles for Kyber operations with concise context to fit page width.}
\label{tab:kyber_median_cycles}
\end{table}


Across all three workloads, cache behavior was uniformly stable. Monitoring via QEMU’s cache plugin confirmed low instruction and data cache miss rates, consistent with compact memory footprints and tight loop-local access patterns. Instruction cache performance benefited from small binary sizes and the absence of dynamic control flow, while data cache pressure remained minimal due to statically sized inputs and short-lived stack allocations. These characteristics consistently indicate compute-bound, not memory-bound, behavior in the native case.

Together, these results confirm that the native execution environment offers a low-noise, deterministic baseline across benchmarks. Variability was minimal, CPU time aligned closely with wall-clock time, memory remained well-bounded, and system-level interference was negligible—making this configuration ideal for isolating performance effects introduced in subsequent secure or enclave-based execution modes.

\section{Enclave Execution Results and Overheads}
\label{sec:enclave-execution}

Building on the native baseline characterization, this section analyzes the performance impacts and system overheads observed when executing representative workloads—Kyber, Dhrystone, and CoreMark—within a secure enclave environment. The experiments were conducted on the identical Low-End Baseline platform (64~MB RAM, single in-order RISC-V core), isolating overheads introduced exclusively by enclave security mechanisms such as memory encryption, integrity verification, and enclave runtime management.

All benchmarks were initially executed ten times sequentially under enclave protection to establish a direct, low-variability comparison against native runs. The Kyber \texttt{kyber\_decaps} operation exhibited median CPU cycle counts consistently around 3.5 million cycles per run (e.g., 3,505,686 cycles in Run~1), approximately 1.2$\times$ the native sequential baseline of about 2.9 million cycles (see Table~\ref{tab:kyber_median_cycles}). Average cycles were slightly higher, around 3.53 million, reflecting stable and repeatable enclave performance. Other key Kyber operations, such as \texttt{indcpa\_keypair} and \texttt{indcpa\_enc}, showed median cycle counts near 1.74 million and 2.62 million cycles respectively, consistent across runs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/kyber_64M_1Core_comparison.pdf}
    \caption{Cycle count comparison for Kyber operations (native vs enclave). Error bars reflect variation over 10 runs.}
    \label{fig:kyber_cycles}
\end{figure}

Corresponding wall-clock times for these Kyber enclave runs averaged approximately 29.3 seconds, significantly exceeding native baselines of 10--11 seconds, primarily due to cryptographic memory encryption and integrity checks that increase the cost of each memory access and instruction fetch.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.7\textwidth]{figures/wall_clock_comparison.pdf}
%    \caption{Wall-clock execution time across benchmarks (native vs enclave, averaged over 10 runs).}
%    \label{fig:wall_clock}
%\end{figure}

Memory footprint across enclave runs remained tightly constrained near 10~KB RSS, mirroring native conditions. Page faults were minimal, with typically one minor page fault and zero major faults per run, indicating effective enclave memory management without paging overhead.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.75\linewidth]{figures/enclave_memory_pagefaults.png}
%   \caption{Resident Set Size (RSS) memory usage and page fault counts during Kyber enclave runs, showing stable memory consumption and minimal paging overhead.}
%   \label{fig:memory_pagefaults}
% \end{figure}

A notable system-level overhead manifested in the form of a dramatic increase in involuntary context switches—approximately 66,000 per run, nearly two orders of magnitude above the roughly 350 switches observed natively. This surge likely reflects enclave runtime activities such as page re-encryption cycles, secure interrupt handling, and frequent enclave scheduler invocations. While these involuntary context switches contribute to wall-clock time variability, user-mode CPU time remained dominant (over 95\% of total CPU time), confirming that computation remains primarily enclave-resident and compute-bound.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.75\linewidth]{figures/context_switches_comparison.png}
%   \caption{Involuntary context switches during native versus enclave execution, highlighting a nearly 200$\times$ increase under enclave protection.}
%   \label{fig:context_switches}
% \end{figure}

The Dhrystone benchmark demonstrated a similar pattern, with wall-clock times averaging 24.7 seconds under enclave protection, roughly 1.6$\times$ slower than the native 15.3-second average. User and system CPU times both rose to near 41 seconds, reflecting the combined cost of enclave runtime overheads and cryptographic memory protection. Throughput, measured in Dhrystone operations per second, declined modestly to the 395,000--408,000 DPS range from a native baseline near 400,000 DPS. Memory consumption and page fault rates stayed consistent with Kyber’s enclave runs, and involuntary context switch counts remained elevated near 66,000 per run.

CoreMark enclave runs averaged 107 seconds wall-clock time—about 1.05$\times$ slower than the 101-second native average—with throughput dropping to 919--939 iterations per second compared to the native 967--988 range. CPU time profiles and memory usage mirrored the trends seen in Kyber and Dhrystone, reinforcing a stable but nontrivial enclave overhead footprint.

Beyond sequential execution, attempts to scale enclave workloads via parallelism revealed critical hardware-enforced constraints. Specifically, running 10 concurrent parallel enclave instances consistently failed due to exhaustion or conflicts in Physical Memory Protection (PMP) region allocation. This limitation manifested as execution aborts or enclave crashes, highlighting the sensitivity of enclave concurrency to limited hardware memory isolation resources.

Reducing parallelism to 5 simultaneous runs yielded improved but still imperfect stability: intermittent failures occurred less frequently, suggesting that PMP regions remain a bottleneck under moderate concurrency levels. Nevertheless, the 5$\times$ parallel configuration was generally capable of completing most benchmark runs.

In contrast, 2$\times$ parallel enclave runs executed reliably with no PMP-related failures, though at the expected overhead cost relative to native concurrency. Additionally, all 10 sequential enclave runs were stable and completed successfully, underscoring sequential execution as the most dependable operational mode given current PMP constraints.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.75\linewidth]{figures/enclave_parallelism_stability.png}
%   \caption{Effect of parallel enclave instances on execution stability. Failures increase sharply beyond 5 concurrent runs due to PMP region limitations.}
%   \label{fig:parallelism_stability}
% \end{figure}

These results underscore a fundamental trade-off in enclave execution scalability: while parallelism can improve throughput, strict hardware PMP enforcement imposes hard limits on enclave concurrency that manifest as failures at higher parallel counts. Addressing this will likely require advances in PMP management, dynamic region allocation, or hardware extensions to support larger concurrent enclave workloads without compromising memory protection guarantees.

Enclave execution imposes moderate but consistent performance overheads relative to native runs, with cycle counts and execution times increasing approximately 20--60\%. Memory usage remains tightly bounded, and paging is effectively avoided. The most substantial system-level overhead is the marked increase in involuntary context switches during enclave operation, which affects timing predictability but not computational correctness.

These findings confirm that secure enclaves can sustain a wide range of computational workloads with predictable overheads and stable memory behavior, while also revealing important hardware-imposed concurrency limitations. The detailed performance and system metric breakdown presented here forms a critical foundation for future optimization and hardware enhancement efforts targeting secure enclave scalability and efficiency.

\section{Workload Sensitivity Analysis (Kyber vs CoreMark vs Dhrystone)}
\label{sec:workload-sensitivity}

This section presents a comparative analysis of enclave-induced overheads across three fundamentally different workload types: cryptographic (Kyber), general-purpose embedded (CoreMark), and integer-heavy synthetic benchmarks (Dhrystone). The goal is to understand how diverse computational patterns react to the constraints and runtime characteristics of trusted enclave execution, particularly under a constrained hardware configuration.

All evaluations were performed on a minimal RISC-V platform comprising a single in-order core and 64~MB of RAM, consistent with prior sections. Each workload was executed both natively and within the secure enclave environment, under identical operating conditions. Enclave runs were repeated ten times sequentially to reduce interference, and measurements were recorded using Linux time utilities and cycle counters embedded in the enclave runtime. System-level statistics, such as context switches, resident set size (RSS), and page fault counts, were also gathered to provide a holistic view of the overheads.

Among the three workloads, Kyber proved to be the most sensitive to enclave-related performance impacts. The median number of CPU cycles for its \texttt{kyber\_decaps} operation increased from approximately 2.9 million in the native case to over 3.5 million when executed inside the enclave—a roughly 1.2$\times$ increase. However, this difference was magnified in terms of wall-clock time: the native execution completed in approximately 10.7 seconds, whereas the enclave version averaged 29.2 seconds, representing a 2.7$\times$ slowdown. This discrepancy suggests that memory encryption and integrity verification add nontrivial latency per memory access, disproportionately affecting workloads with dense memory usage or pointer-heavy data structures, which are characteristic of lattice-based cryptographic primitives like Kyber.

In stark contrast, CoreMark exhibited the least performance degradation under enclave execution. Wall-clock time increased only slightly, from a native average of 101.5 seconds to 107.3 seconds in the enclave, corresponding to a 1.06$\times$ overhead. This minimal slowdown is attributed to CoreMark’s balanced workload structure, combining arithmetic operations, control logic, and memory accesses in a manner that is less sensitive to cryptographic memory protection. CoreMark’s throughput declined modestly, from a native range of 967--988 iterations per second to an enclave-protected range of 919--939, indicating that while overheads exist, they are amortized over the longer duration and lower frequency of secure memory operations.

Dhrystone occupied a middle ground between Kyber and CoreMark in terms of sensitivity. The wall-clock time for Dhrystone increased from 15.3 seconds natively to 24.7 seconds under enclave protection, yielding an overhead factor of approximately 1.6$\times$. Despite its primary reliance on integer arithmetic and control structures, Dhrystone is sensitive to memory system performance due to its short execution time and tight loops. The secure memory mechanisms in the enclave added consistent latency, resulting in degraded throughput (395,000--408,000 Dhrystone operations per second) when compared to the native baseline of approximately 400,000 DPS. These results confirm that even relatively simple benchmarks experience measurable slowdown under enclave protection, though to a lesser extent than cryptographic workloads.

Across all three benchmarks, system-level metrics revealed a consistent pattern in enclave execution behavior. In particular, the number of involuntary context switches per run consistently exceeded 66,000 for each workload. This value is nearly two orders of magnitude greater than in native runs, where fewer than 400 involuntary context switches were recorded. The uniformity of this value across distinct workloads indicates that the overhead is primarily systemic, driven by the enclave runtime itself rather than specific application behavior. These context switches are likely associated with secure page handling, timer interrupts, and asynchronous enclave runtime maintenance operations. Despite their frequency, the impact appears bounded and does not significantly impair CPU-bound computation, as confirmed by the preservation of user-mode CPU time dominance in all executions.

Memory behavior across enclave runs remained remarkably stable and comparable to native execution. All workloads reported a resident set size (RSS) close to 10~KB, with no major page faults and at most one minor fault per run. This result indicates that the enclave’s memory allocator and secure paging subsystem are sufficiently robust to handle working sets without inducing swapping or paging penalties, even under constrained RAM conditions.

These observations are summarized in Table~\ref{tab:sensitivity-overheads}, which compares native and enclave wall-clock times, overhead factors, and context switch counts across all three benchmarks. Figure~\ref{fig:workload_overhead} visually contrasts the wall-clock overhead imposed by enclave execution, highlighting Kyber’s pronounced sensitivity and CoreMark’s relative resilience.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|p{3.5cm}|}
\hline
\textbf{Benchmark} & \textbf{Native Time (s)} & \textbf{Enclave Time (s)} & \textbf{Overhead Factor} & \textbf{Involuntary CS} \\
\hline
Kyber     & 10.7   & 29.2   & 2.73$\times$  & 66,000 \\
\hline
Dhrystone & 15.3   & 24.7   & 1.61$\times$  & 66,000 \\
\hline
CoreMark  & 101.5  & 107.3  & 1.06$\times$  & 66,000 \\
\hline
\end{tabular}
\caption{Enclave Execution Sensitivity Across Workloads. Overhead factors indicate slowdown relative to native execution.}
\label{tab:sensitivity-overheads}
\end{table}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.7\linewidth]{figures/workload_overhead_comparison.png}
%   \caption{Relative increase in wall-clock execution time under enclave protection across three workload types.}
%   \label{fig:workload_overhead}
% \end{figure}

In summary, this workload sensitivity analysis reveals a clear hierarchy of enclave overhead impacts based on workload characteristics. Cryptographic workloads, particularly those like Kyber with memory-intensive operations and frequent data-dependent access patterns, are disproportionately affected by the secure memory fabric’s latency and the enclave runtime’s background operations. In contrast, workloads with predictable access patterns or longer execution durations, such as CoreMark, exhibit substantially lower performance degradation. These insights are critical for system designers seeking to deploy trusted execution environments in resource-constrained systems, as they inform trade-offs between security, performance, and workload type.

\section{Impact of Hardware Parameters (Cores, Memory, Cache)}

To comprehensively assess how hardware configurations affect enclave performance, we systematically varied CPU core counts, memory allocations, and cache configurations.

Scaling the number of CPU cores yielded anticipated improvements for parallel workloads, with throughput gains plateauing beyond six cores due to synchronization overhead and increased enclave management complexity. Sequential workloads such as Kyber remained unaffected by core scaling.

Memory allocation experiments revealed that increasing memory size from 64MB to 2GB improved performance for memory-intensive benchmarks by reducing paging and increasing cache hit rates. However, beyond approximately 512MB, gains diminished, indicating sufficient memory provision for the workload’s working set.

Cache configuration impacts were evaluated using the QEMU cache modeling plugin. Larger cache sizes and higher associativity reduced miss rates, particularly for instruction-heavy workloads like Dhrystone. While the plugin’s system-wide scope limits insights into enclave-specific cache activity, the trends suggest that cache tuning can meaningfully influence enclave performance.

Importantly, the relationship between memory allocation and PMP usage is nuanced. Larger enclave memory regions may require multiple PMP entries if the memory cannot be described contiguously in a single PMP register, effectively reducing the total number of enclaves supported simultaneously. This interaction between memory layout and PMP capacity represents a critical consideration for enclave deployment strategies.
