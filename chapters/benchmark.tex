\chapter{Benchmarking Results and Observations}
\label{chap:benchmarking}

This chapter provides a detailed empirical performance evaluation of a set of benchmark workloads executed within the Keystone enclave framework. The primary objective is to quantitatively assess the overheads and limitations introduced by enclave-based execution, focusing on how system-level parameters and hardware constraints influence workload behavior under trusted execution.

The evaluation covers a mix of general-purpose and cryptographic workloads. Specifically, we utilize the Dhrystone and CoreMark benchmarks, which are standard tools for measuring integer processing performance and embedded system efficiency, respectively. Additionally, the Kyber post-quantum cryptographic algorithm is included to evaluate a compute-intensive, security-critical workload relevant for future-proof cryptographic applications. This combination allows us to characterize enclave performance across a spectrum of computational patterns and resource demands.

\section{Baseline: Native (Non-Enclave) Performance}

To establish a baseline for evaluating enclave overheads, we first executed all benchmarks natively on the simulated RISC-V platform without any enclave isolation. This phase allows us to measure the raw performance characteristics of the system and isolate the costs introduced solely by secure execution.

The Dhrystone and CoreMark benchmarks exhibited expected scaling behavior with respect to CPU cores and memory size. As CPU core count increased, throughput for these parallelizable benchmarks improved approximately linearly, reflecting efficient multi-threaded execution on the underlying hardware. Memory size impacted CoreMark more noticeably, particularly during memory-intensive phases, where increased memory reduced paging and improved cache hit rates. In contrast, Dhrystone’s smaller working set resulted in negligible sensitivity to memory size variations.

The Kyber cryptographic benchmark behaved differently. Due to its inherently sequential algorithmic structure, Kyber’s execution time remained relatively constant across changes in CPU core count. Likewise, memory size had minimal impact on Kyber performance, as its working set comfortably fit within allocated memory without inducing paging.

These native baseline results confirm the stability and predictability of the simulated system environment, providing a solid foundation for subsequent enclave overhead analysis.

\section{Enclave Execution Results and Overheads}

Execution within Keystone enclaves introduces overhead from multiple sources: memory isolation enforced by PMP, additional context switches to manage transitions between secure and non-secure execution states, and the general overhead of enforcing enclave boundaries at runtime.

Across all benchmarks, enclave execution resulted in increased execution times compared to native runs. Overhead magnitude varied by workload and system configuration but generally remained below 15\%. For parallel workloads such as CoreMark, overhead tended to increase with the number of CPU cores, likely due to increased synchronization demands and frequent enclave exit and re-entry operations during thread management. Dhrystone showed more modest overhead growth with core scaling.

Cache behavior, measured indirectly via QEMU’s cache modeling plugin, indicated modest increases in cache miss rates during enclave execution. Although this plugin provides system-wide cache metrics rather than enclave-specific data, the observed increases likely stem from PMP-related overheads such as cache flushing or restricted memory access patterns. These overheads are an expected consequence of maintaining strong memory isolation guarantees.

A critical observation during enclave execution was the enforcement and limitation of PMP resources. The fixed PMP register count on the QEMU virt platform fundamentally restricts the number of enclaves that can be concurrently instantiated. When the number of requested enclaves exceeds the 16 PMP entries, enclave creation fails. These failures are characterized by SBI call error codes and logged kernel errors indicating that no PMP registers are available. This limitation underscores the importance of considering PMP capacity in enclave orchestration and resource allocation.

\section{Workload Sensitivity Analysis (Kyber vs CoreMark vs Dhrystone)}

The Kyber post-quantum cryptographic algorithm is a representative workload for evaluating the performance impact of enclave execution on security-critical applications. Kyber consists of three primary operations—key generation, encapsulation, and decapsulation—each profiled extensively within enclaves using the Keystone timer API to capture precise cycle counts.

Execution within enclaves increased Kyber’s runtime by approximately 7–9\% compared to native execution. This overhead is modest given the cryptographic operations’ complexity and the security benefits provided by enclaves.

As with native execution, Kyber performance was insensitive to increases in CPU core count due to its inherently sequential design. Memory usage during enclave execution remained stable, with no indications of paging or contention. System-wide cache miss rates observed through QEMU remained consistent with Kyber’s memory access patterns, including phases dominated by number-theoretic transforms and polynomial arithmetic.

From a resource perspective, the requirement of one PMP entry per enclave constrains the number of parallel Kyber instances that can be simultaneously deployed. This constraint must be taken into account when designing systems requiring multiple isolated cryptographic operations.

\section{Impact of Hardware Parameters (Cores, Memory, Cache)}

To comprehensively assess how hardware configurations affect enclave performance, we systematically varied CPU core counts, memory allocations, and cache configurations.

Scaling the number of CPU cores yielded anticipated improvements for parallel workloads, with throughput gains plateauing beyond six cores due to synchronization overhead and increased enclave management complexity. Sequential workloads such as Kyber remained unaffected by core scaling.

Memory allocation experiments revealed that increasing memory size from 64MB to 2GB improved performance for memory-intensive benchmarks by reducing paging and increasing cache hit rates. However, beyond approximately 512MB, gains diminished, indicating sufficient memory provision for the workload’s working set.

Cache configuration impacts were evaluated using the QEMU cache modeling plugin. Larger cache sizes and higher associativity reduced miss rates, particularly for instruction-heavy workloads like Dhrystone. While the plugin’s system-wide scope limits insights into enclave-specific cache activity, the trends suggest that cache tuning can meaningfully influence enclave performance.

Importantly, the relationship between memory allocation and PMP usage is nuanced. Larger enclave memory regions may require multiple PMP entries if the memory cannot be described contiguously in a single PMP register, effectively reducing the total number of enclaves supported simultaneously. This interaction between memory layout and PMP capacity represents a critical consideration for enclave deployment strategies.
