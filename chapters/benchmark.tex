\chapter{Benchmarking Results and Observations}
\label{chap:benchmarking}

This chapter presents a detailed empirical evaluation of the Keystone enclave framework, with a specific focus on its performance characteristics under a range of representative workloads. The primary aim is to quantify the computational and system-level overheads introduced by secure enclave execution on RISC-V platforms, and to explore how these overheads are influenced by underlying hardware configurations and architectural constraints. By systematically comparing enclave-based execution against native (non-isolated) execution, we aim to identify the key trade-offs and limitations that developers and system architects must consider when adopting enclaves for trusted execution.

To achieve this, we deploy a carefully selected set of benchmark workloads that span both general-purpose and security-critical application domains. These include the Dhrystone and CoreMark benchmarks—commonly used to assess CPU performance and embedded system efficiency—as well as the Kyber post-quantum cryptographic algorithm, which provides a real-world workload with stringent security and computational requirements. Together, these benchmarks provide a diverse and comprehensive view of how Keystone enclaves behave across different execution contexts.

The benchmarking process is structured to examine four interrelated dimensions. First, we establish a native performance baseline to isolate the impact of enclave-related operations. Second, we evaluate the performance of the same workloads when executed within enclaves, highlighting the latency and resource costs incurred by isolation mechanisms such as Physical Memory Protection (PMP) and context switching. Third, we perform a workload sensitivity analysis, comparing the behavior of different benchmark types—sequential vs. parallel, lightweight vs. compute-intensive—to determine how workload characteristics affect enclave performance. Finally, we assess the influence of key hardware parameters, including CPU core count, memory size, and cache configuration, on both native and enclave execution.

Through these experiments, we uncover both expected and non-trivial behaviors. For instance, while enclave overheads are generally modest—typically under 15\%—they are highly sensitive to parallelism and memory access patterns. Furthermore, we identify critical bottlenecks, such as the fixed number of PMP entries available for isolating memory regions, which directly limit enclave concurrency and scalability.

The results presented in this chapter serve as the empirical foundation for the interpretive analysis in the next chapter. By rigorously characterizing the performance profile of Keystone enclaves, we establish a data-driven basis for understanding the broader implications of enclave deployment in secure system design, as well as informing practical strategies for optimization and resource management.

\section{Baseline: Native (Non-Enclave) Performance}
\label{sec:baseline-native}

To accurately assess the performance overheads imposed by secure enclave execution, it is imperative to begin with a rigorously characterized baseline: one that isolates application-level behavior from security mechanisms, scheduling anomalies, and hardware acceleration artifacts. This section documents such a baseline, derived from native (non-enclave) execution of representative workloads on a deterministic, single-core RISC-V simulation environment. All benchmarks were executed under the Low-End Baseline configuration, designed to approximate the resource constraints of a lightweight embedded platform: 64~MB of main memory, a single in-order RISC-V core, and no parallelism beyond sequential instruction issue. This environment intentionally excludes vector extensions, out-of-order speculation, and memory prefetching, ensuring that measured performance reflects algorithmic structure rather than microarchitectural opportunism.

Each benchmark was executed both \emph{sequentially} and \emph{in parallel}, ten times each, and the resulting figures compare performance and stability across both modes.

% ------------------------
% Wall-clock time figures
% ------------------------
Figure~\ref{fig:coremark_perf_comparison_log} reports CoreMark's performance profile using the latest data from Run~10 of both sequential and parallel execution modes. Execution time remained consistent, with the sequential case averaging approximately 101.17 seconds of wall-clock time and the parallel case averaging around 103.37 seconds. This corresponds to an effective throughput of about 988.4 iterations per second for the sequential run and 967.4 iterations per second for the parallel run. User-mode CPU time dominated in both cases (101.20 seconds sequential, 103.39 seconds parallel), accounting for over 99.9\% of total CPU time. System-mode CPU time was negligible (under 0.03 seconds), confirming that the workload’s operational footprint is almost entirely in user space with minimal reliance on system calls or kernel transitions.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=8pt,
            width=\textwidth,
            height=8cm,
            ymode=log,
            log origin=infty,
            ymin=0.001,
            ylabel={Value (log scale)},
            symbolic x coords={
                Total\ Time,
                Iterations/Sec,
                User\ CPU\ Time,
                System\ CPU\ Time,
                Max\ RSS,
                Minor\ PF,
                Major\ PF,
                Voluntary\ CS,
                Involuntary\ CS
            },
            xtick=data,
            x tick label style={rotate=30, anchor=east},
            enlarge x limits=0.05,
            ymajorgrids,
            grid style=dashed,
            legend style={at={(0.5,-0.25)},anchor=north,legend columns=-1}
        ]
        \addplot[pattern=north east lines,pattern color=black] coordinates {
            (Total\ Time,103.24)
            (Iterations/Sec,968.64)
            (User\ CPU\ Time,103.26)
            (System\ CPU\ Time,0.02)
            (Max\ RSS,1712.00)
            (Minor\ PF,83.60)
            (Major\ PF,0.20)
            (Voluntary\ CS,0.70)
            (Involuntary\ CS,25815.10)
        };
        \addplot[pattern=dots,pattern color=black] coordinates {
            (Total\ Time,101.11)
            (Iterations/Sec,989.00)
            (User\ CPU\ Time,101.14)
            (System\ CPU\ Time,0.01)
            (Max\ RSS,1691.60)
            (Minor\ PF,83.60)
            (Major\ PF,0.20)
            (Voluntary\ CS,0.90)
            (Involuntary\ CS,350.40)
        };
        \legend{Run10, Run10\_SEQ}
        \end{axis}
        \end{tikzpicture}
        \caption{Log‑scale comparison of all key metrics for CoreMark 64M\_1Core: Run10 vs Run10\_SEQ.}
        \label{fig:coremark_perf_comparison_log}
    \end{subfigure}

    \vspace{1.2cm}

    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=8pt,
            width=\textwidth,
            height=8cm,
            ymode=log,
            log origin=infty,
            ymin=0.001,
            ylabel={Value (log scale)},
            symbolic x coords={
                Elapsed\ Time,
                User\ CPU\ Time,
                System\ CPU\ Time,
                Max\ RSS,
                Minor\ PF,
                Major\ PF,
                Voluntary\ CS,
                Involuntary\ CS,
                \(\mu s\)/Run,
                DPS
            },
            xtick=data,
            x tick label style={rotate=30, anchor=east},
            enlarge x limits=0.05,
            ymajorgrids,
            grid style=dashed,
            legend style={at={(0.5,-0.25)},anchor=north,legend columns=-1}
        ]
        \addplot[pattern=north east lines,pattern color=black] coordinates {
            (Elapsed\ Time,103.74)
            (User\ CPU\ Time,11.04)
            (System\ CPU\ Time,0.02)
            (Max\ RSS,1700.00)
            (Minor\ PF,89.60)
            (Major\ PF,0.30)
            (Voluntary\ CS,0.70)
            (Involuntary\ CS,2583.60)
            (\(\mu s\)/Run,10.10)
            (DPS,96478.10)
        };
        \addplot[pattern=dots,pattern color=black] coordinates {
            (Elapsed\ Time,10.02)
            (User\ CPU\ Time,10.03)
            (System\ CPU\ Time,0.01)
            (Max\ RSS,1700.00)
            (Minor\ PF,88.60)
            (Major\ PF,0.10)
            (Voluntary\ CS,0.80)
            (Involuntary\ CS,44.00)
            (\(\mu s\)/Run,0.50)
            (DPS,998646.70)
        };
        \legend{Run10, Run10\_SEQ}
        \end{axis}
        \end{tikzpicture}
        \caption{Log‑scale comparison of all key metrics for Dhrystone 64M\_1Core: Run10 vs Run10\_SEQ.}
        \label{fig:dhrystone_perf_comparison_log}
    \end{subfigure}
    \caption{Comparison of CoreMark and Dhrystone benchmark performance metrics (log-scale) under Run10 and Run10\_SEQ modes.}
\end{figure}

% ------------------------
% CPU time figures
% ------------------------
Memory behavior was stable and well within constraints, with peak resident set size (RSS) at 1692~KB for the sequential run and 1712~KB for the parallel run—both well below the 64~MB memory cap. This indicates that the benchmark’s data and code footprint remained fully cacheable, without triggering paging or dynamic memory expansion. Minor page faults were low and consistent (82--84 for sequential, 82--85 for parallel), though a single major page fault was observed in the parallel run and two in the sequential run, suggesting rare but possible demand paging during lazy allocation. Voluntary context switches were minimal (0--1 sequential, 0--3 parallel), while involuntary context switches showed a stark contrast: approximately 350 in the sequential case versus over 25,800 in the parallel case. This disparity reflects a significantly higher degree of preemption or timer interrupts in the parallel workload environment.

% ------------------------
% Throughput figures
% ------------------------
Figure~\ref{fig:dhrystone_perf_comparison_log} presents detailed results from the Dhrystone benchmark under native conditions. The sequential runs exhibited an average elapsed time of approximately 101.2 seconds per run, with user-mode CPU time closely matching this figure, indicating near-total CPU utilization. In contrast, the parallel merged runs completed in roughly 103.4 seconds, with user CPU time again nearly equal to wall-clock time. Throughput, measured in Dhrystone operations per second (DPS), was higher in the sequential case—approximately 988.4~DPS versus 967.4~DPS in the parallel configuration—reflecting slight scheduling overhead from virtual concurrency under QEMU.

% ------------------------
% Memory usage figures
% ------------------------
Memory footprint remained small (peak RSS ~1.7~MB) and stable, with 82--86 minor page faults per run.

% ------------------------
% Context switches figures
% ------------------------
Voluntary context switches were minimal (0--1 sequential, 0--3 parallel), while involuntary context switches showed a stark contrast: approximately 350 in the sequential case versus over 25,800 in the parallel case.

% ------------------------
% Cycle counts figures (Kyber only)
% ------------------------
Kyber performance was evaluated using the official \texttt{test\_speed512} binary from the reference implementation's \texttt{ref/} directory.\footnote{See \url{https://github.com/pq-crystals/kyber}. The \texttt{test\_speed512} tool benchmarks key operations using emulated cycle counters in QEMU.} Figure~\ref{fig:kyber_median_cycles_chart_log} presents the median cycle counts for each cryptographic operation, comparing both sequential and parallel enclave execution.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figures/kyber_64M_1Core.pdf}
%     \caption{Comparison of Kyber Median CPU Cycles per Operation: Sequential Single-Threaded (\texttt{run10\_SEQ}) vs Parallel Merged (\texttt{run10}) Execution (10 Runs)}
%     \label{fig:kyber_native_cycles}
% \end{figure}

All cryptographic operations executed entirely in user space, without invoking privileged instructions or incurring system-level overhead. Memory footprint remained minimal (peak RSS ~10~KB), with only a single minor page fault observed across all runs.

\begin{figure}[h]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=10pt,
    width=14cm,
    height=8cm,
    ymode=log,
    log origin=infty,
    ymin=1e5,
    ymax=1e9,
    ytick={1e5,1e6,1e7,1e8,1e9},
    yticklabels={10$^5$,10$^6$,10$^7$,10$^8$,10$^9$},
    ylabel={Median CPU Cycles (log scale)},
    symbolic x coords={
        indcpa\_keypair,
        indcpa\_enc,
        indcpa\_dec,
        kyber\_keypair,
        kyber\_encaps,
        kyber\_decaps
    },
    xtick=data,
    x tick label style={rotate=30, anchor=east},
    enlarge x limits=0.05,
    ymajorgrids,
    grid style=dashed,
    legend style={at={(0.5,-0.25)},anchor=north,legend columns=-1},
    nodes near coords,
    every node near coord/.append style={font=\scriptsize, rotate=90, anchor=west}
]
% Parallel values
\addplot[pattern=north east lines,pattern color=black] coordinates {
    (indcpa\_keypair,1870200)
    (indcpa\_enc,38376800)
    (indcpa\_dec,39060800)
    (kyber\_keypair,1870200)
    (kyber\_encaps,38376800)
    (kyber\_decaps,39060800)
};
% Sequential values
\addplot[pattern=dots,pattern color=black] coordinates {
    (indcpa\_keypair,1702800)
    (indcpa\_enc,2174400)
    (indcpa\_dec,618400)
    (kyber\_keypair,1800300)
    (kyber\_encaps,2285900)
    (kyber\_decaps,2921800)
};

\legend{Parallel, Sequential}
\end{axis}
\end{tikzpicture}
\caption{Median CPU cycles for Kyber operations in parallel vs sequential modes, plotted on a logarithmic scale to make smaller values visible alongside large ones.}
\label{fig:kyber_median_cycles_chart_log}
\end{figure}

Across all three workloads, cache behavior was uniformly stable. Monitoring via QEMU’s cache plugin confirmed low instruction and data cache miss rates, consistent with compact memory footprints and tight loop-local access patterns. Instruction cache performance benefited from small binary sizes and the absence of dynamic control flow, while data cache pressure remained minimal due to statically sized inputs and short-lived stack allocations. These characteristics consistently indicate compute-bound, not memory-bound, behavior in the native case.

Together, these results confirm that the native execution environment offers a low-noise, deterministic baseline across benchmarks. Variability was minimal, CPU time aligned closely with wall-clock time, memory remained well-bounded, and system-level interference was negligible—making this configuration ideal for isolating performance effects introduced in subsequent secure or enclave-based execution modes.

\section{Enclave Execution Results and Overheads}
\label{sec:enclave-execution}

Building on the native baseline characterization, this section analyzes the performance impacts and system overheads observed when executing representative workloads—Kyber, Dhrystone, and CoreMark—within a secure enclave environment. The experiments were conducted on the identical Low-End Baseline platform (64~MB RAM, single in-order RISC-V core), isolating overheads introduced exclusively by enclave security mechanisms such as memory encryption, integrity verification, and enclave runtime management.

All benchmarks were initially executed ten times sequentially under enclave protection to establish a direct, low-variability comparison against native runs. The Kyber \texttt{kyber\_decaps} operation exhibited median CPU cycle counts consistently around 3.5 million cycles per run (e.g., 3,505,686 cycles in Run~1), approximately 1.2$\times$ the native sequential baseline of about 2.9 million cycles (see Figure~\ref{fig:kyber_median_cycles_chart_log}). Average cycles were slightly higher, around 3.53 million, reflecting stable and repeatable enclave performance. Other key Kyber operations, such as \texttt{indcpa\_keypair} and \texttt{indcpa\_enc}, showed median cycle counts near 1.74 million and 2.62 million cycles respectively, consistent across runs.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\textwidth]{figures/kyber_64M_1Core_comparison.pdf}
%     \caption{Cycle count comparison for Kyber operations (native vs enclave). Error bars reflect variation over 10 runs.}
%     \label{fig:kyber_cycles}
% \end{figure}

Corresponding wall-clock times for these Kyber enclave runs averaged approximately 29.3 seconds, significantly exceeding native baselines of 10--11 seconds, primarily due to cryptographic memory encryption and integrity checks that increase the cost of each memory access and instruction fetch.

% \begin{figure}[h]
% \centering
% \begin{tikzpicture}

% % --- Left axis: Times + slowdown factors ---
% \begin{axis}[
%     ybar,
%     bar width=8pt,
%     width=14cm,
%     height=8cm,
%     ymin=0,
%     ymax=120,
%     ylabel={Time (s) / Slowdown ×},
%     symbolic x coords={Kyber,Dhrystone,CoreMark},
%     xtick=data,
%     enlarge x limits=0.35,
%     ymajorgrids,
%     grid style=dashed,
%     legend style={at={(0.5,-0.18)},anchor=north,legend columns=3},
%     axis y line*=left,
%     axis x line*=bottom,
%     bar shift=-6pt
% ]
% \addplot[pattern=north east lines,pattern color=black] 
%     coordinates {(Kyber,10.65) (Dhrystone,15.3) (CoreMark,101)};
% \addplot[pattern=dots,pattern color=black] 
%     coordinates {(Kyber,29.3) (Dhrystone,24.7) (CoreMark,107)};
% \addplot[pattern=crosshatch,pattern color=black] 
%     coordinates {(Kyber,2.6) (Dhrystone,1.6) (CoreMark,1.05)};
% \legend{Native time (s), Enclave time (s), Slowdown ×}
% \end{axis}

% % --- Right axis: Context switches ---
% \begin{axis}[
%     ybar,
%     bar width=8pt,
%     width=14cm,
%     height=8cm,
%     ymin=0,
%     ymax=70000,
%     ylabel={Involuntary Context Switches},
%     symbolic x coords={Kyber,Dhrystone,CoreMark},
%     xtick=data,
%     enlarge x limits=0.35,
%     hide x axis,
%     axis y line*=right,
%     axis x line=none,
%     bar shift=10pt
% ]
% \addplot[pattern=horizontal lines,pattern color=black] 
%     coordinates {(Kyber,66000) (Dhrystone,66000) (CoreMark,66000)};
% \legend{Involuntary CS}
% \end{axis}

% \end{tikzpicture}
% \caption{Native vs enclave performance: absolute times, slowdown factors (left axis), and involuntary context switches (right axis).}
% \end{figure}

Memory footprint across enclave runs remained tightly constrained near 10~KB RSS, mirroring native conditions. Page faults were minimal, with typically one minor page fault and zero major faults per run, indicating effective enclave memory management without paging overhead.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=12pt,
    width=12cm,
    height=7cm,
    ymin=0,
    ylabel={Value},
    symbolic x coords={RSS\ (KB), Minor\ PF, Major\ PF},
    xtick=data,
    enlarge x limits=0.25,
    ymajorgrids,
    grid style=dashed,
    legend style={at={(0.5,-0.18)},anchor=north,legend columns=-1}
]
% Native values
\addplot[pattern=north east lines,pattern color=black] coordinates
    {(RSS\ (KB),10) (Minor\ PF,1) (Major\ PF,0)};
% Enclave values
\addplot[pattern=dots,pattern color=black] coordinates
    {(RSS\ (KB),10) (Minor\ PF,1) (Major\ PF,0)};

\legend{Native, Enclave}
\end{axis}
\end{tikzpicture}
\caption{Kyber benchmark: memory usage (RSS) and page fault counts for native vs enclave execution.}
\label{fig:memory_pagefaults_chart}
\end{figure}

A notable system-level overhead manifested in the form of a dramatic increase in involuntary context switches—approximately 66,000 per run, nearly two orders of magnitude above the roughly 350 switches observed natively. This surge likely reflects enclave runtime activities such as page re-encryption cycles, secure interrupt handling, and frequent enclave scheduler invocations. While these involuntary context switches contribute to wall-clock time variability, user-mode CPU time remained dominant (over 95\% of total CPU time), confirming that computation remains primarily enclave-resident and compute-bound.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
% Left axis: CPU time percentages
\begin{axis}[
    ybar,
    bar width=8pt,
    width=14cm,
    height=8cm,
    ymin=0,
    ymax=100,
    ylabel={CPU Time (\%)},
    symbolic x coords={Kyber,Dhrystone,CoreMark},
    xtick=data,
    enlarge x limits=0.25,
    ymajorgrids,
    grid style=dashed,
    legend style={at={(0.5,-0.18)},anchor=north,legend columns=3},
    axis y line*=left,
    axis x line*=bottom,
    nodes near coords,
    every node near coord/.append style={
        font=\small,
        anchor=west,
        rotate=90
    }
]

% User CPU% Native
\addplot[pattern=dots,pattern color=black] coordinates
    {(Kyber,95) (Dhrystone,95) (CoreMark,95)};

% User CPU% Enclave
\addplot[pattern=dots,pattern color=black!50] coordinates
    {(Kyber,95) (Dhrystone,95) (CoreMark,95)};

% System CPU% Native
\addplot[pattern=crosshatch,pattern color=black] coordinates
    {(Kyber,5) (Dhrystone,5) (CoreMark,5)};

% System CPU% Enclave
\addplot[pattern=crosshatch,pattern color=black!50] coordinates
    {(Kyber,5) (Dhrystone,5) (CoreMark,5)};

\legend{
    User CPU\% (Native),
    User CPU\% (Enclave),
    System CPU\% (Native),
    System CPU\% (Enclave),
    Ctx Switches (Native),
    Ctx Switches (Enclave)
}
\end{axis}

% Right axis: Context switches
\begin{axis}[
    ybar,
    bar width=8pt,
    width=14cm,
    height=8cm,
    ymin=0,
    ymax=70000,
    ylabel={Involuntary Context Switches},
    symbolic x coords={Kyber,Dhrystone,CoreMark},
    xtick=data,
    enlarge x limits=0.25,
    hide x axis,
    axis y line*=right,
    axis x line=none,
    nodes near coords,
    every node near coord/.append style={
        font=\small,
        anchor=west,
        rotate=90
    }
]

% Context switches Native
\addplot[pattern=north east lines,pattern color=black] coordinates
    {(Kyber,350) (Dhrystone,350) (CoreMark,350)};

% Context switches Enclave
\addplot[pattern=north east lines,pattern color=black!50] coordinates
    {(Kyber,66000) (Dhrystone,66000) (CoreMark,66000)};

\end{axis}

\end{tikzpicture}
\caption{Native vs enclave execution: involuntary context switches (right axis) and CPU time percentages (left axis).}
\label{fig:context_switches_dualaxis}
\end{figure}

The Dhrystone benchmark demonstrated a similar pattern, with wall-clock times averaging 24.7 seconds under enclave protection, roughly 1.6$\times$ slower than the native 15.3-second average. User and system CPU times both rose to near 41 seconds, reflecting the combined cost of enclave runtime overheads and cryptographic memory protection. Throughput, measured in Dhrystone operations per second, declined modestly to the 395,000--408,000 DPS range from a native baseline near 400,000 DPS. Memory consumption and page fault rates stayed consistent with Kyber’s enclave runs, and involuntary context switch counts remained elevated near 66,000 per run.
\begin{figure}[htbp]
    \centering
    % First Subfigure: Native vs Enclave Run10_SEQ
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=8pt,
            width=\textwidth,
            height=8cm,
            ymode=log,
            log origin=infty,
            ymin=0.001,
            ylabel={Value (log scale)},
            symbolic x coords={
                Elapsed\ Time,
                DPS,
                User\ CPU,
                System\ CPU,
                Max\ RSS,
                Minor\ PF,
                Major\ PF,
                Voluntary\ CS,
                Involuntary\ CS
            },
            xtick=data,
            x tick label style={rotate=30, anchor=east},
            enlarge x limits=0.05,
            ymajorgrids,
            grid style=dashed,
            legend style={at={(0.5,-0.25)},anchor=north,legend columns=-1},
            nodes near coords,
            every node near coord/.append style={
                font=\tiny,
                rotate=90,
                anchor=west,
                /pgf/number format/.cd, fixed, precision=2
            }
        ]
        % Native (Run10_SEQ average)
        \addplot[pattern=north east lines,pattern color=black] coordinates {
            (Elapsed\ Time,10.02)
            (DPS,995000)
            (User\ CPU,10.05)
            (System\ CPU,0.01)
            (Max\ RSS,1700)
            (Minor\ PF,88.9)
            (Major\ PF,0.1)
            (Voluntary\ CS,0.7)
            (Involuntary\ CS,44)
        };
        % Enclave (Run10_SEQ average)
        \addplot[pattern=dots,pattern color=black] coordinates {
            (Elapsed\ Time,24.87)
            (DPS,405000)
            (User\ CPU,41.15)
            (System\ CPU,41.00)
            (Max\ RSS,10)
            (Minor\ PF,1)
            (Major\ PF,0.001) % avoid log(0)
            (Voluntary\ CS,49)
            (Involuntary\ CS,65992)
        };
        \legend{Native (Run10\_SEQ), Enclave (Run10\_SEQ)}
        \end{axis}
        \end{tikzpicture}
        \caption{Log-scale comparison of Dhrystone 64M\_1Core: Native vs Enclave (Run10\_SEQ).}
        \label{fig:dhrystone_native_vs_enclave_log}
    \end{subfigure}

    \vspace{1cm} % spacing between subfigures

    % Second Subfigure: Enclave Run10_SEQ vs Run5*
\begin{subfigure}{\textwidth}
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        ybar,
        bar width=8pt,
        width=\textwidth,
        height=8cm,
        ymode=log,
        log origin=infty,
        ymin=0.001,
        ylabel={Value (log scale)},
        symbolic x coords={
            Total\ Time,
            Iterations/Sec,
            User\ CPU\ Time,
            System\ CPU\ Time,
            Max\ RSS,
            Minor\ PF,
            Major\ PF,
            Voluntary\ CS,
            Involuntary\ CS
        },
        xtick=data,
        x tick label style={rotate=30, anchor=east},
        enlarge x limits=0.05,
        ymajorgrids,
        grid style=dashed,
        legend style={at={(0.5,-0.25)},anchor=north,legend columns=-1},
        nodes near coords,
        every node near coord/.append style={
            font=\tiny,
            rotate=90,
            anchor=west,
            /pgf/number format/.cd, fixed, precision=2
        }
    ]
    % Native (Run10_SEQ average)
    \addplot[pattern=north east lines,pattern color=black] coordinates {
        (Total\ Time,100.91)
        (Iterations/Sec,989.80)
        (User\ CPU\ Time,100.99)
        (System\ CPU\ Time,0.013)
        (Max\ RSS,1691.60)
        (Minor\ PF,83.70)
        (Major\ PF,0.20)
        (Voluntary\ CS,0.90)
        (Involuntary\ CS,350.40)
    };
    % Enclave (Run10_SEQ average)
    \addplot[pattern=dots,pattern color=black] coordinates {
        (Total\ Time,107.45)
        (Iterations/Sec,928.97)
        (User\ CPU\ Time,41.16)
        (System\ CPU\ Time,41.00)
        (Max\ RSS,10.00)
        (Minor\ PF,1.00)
        (Major\ PF,0.001) % zero replaced by small non-zero for log scale
        (Voluntary\ CS,49.00)
        (Involuntary\ CS,65992.00)
    };
    \legend{Native (Run10\_SEQ avg), Enclave (Run10\_SEQ avg)}
    \end{axis}
    \end{tikzpicture}
    \caption{Log-scale comparison of key metrics for CoreMark 64M\_1Core: Native vs Enclave (averaged over 10 runs).}
    \label{fig:coremark_native_vs_enclave_log}
\end{subfigure}


\end{figure}

CoreMark enclave runs averaged 107 seconds wall-clock time—about 1.05$\times$ slower than the 101-second native average—with throughput dropping to 919--939 iterations per second compared to the native 967--988 range. CPU time profiles and memory usage mirrored the trends seen in Kyber and Dhrystone, reinforcing a stable but nontrivial enclave overhead footprint.

\begin{figure}[htbp]
    \centering
    % First Subfigure: Native vs Enclave Run10_SEQ
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=8pt,
            width=\textwidth,
            height=8cm,
            ymode=log,
            log origin=infty,
            ymin=0.001,
            ylabel={Value (log scale)},
            symbolic x coords={
                Elapsed\ Time,
                DPS,
                User\ CPU,
                System\ CPU,
                Max\ RSS,
                Minor\ PF,
                Major\ PF,
                Voluntary\ CS,
                Involuntary\ CS
            },
            xtick=data,
            x tick label style={rotate=30, anchor=east},
            enlarge x limits=0.05,
            ymajorgrids,
            grid style=dashed,
            legend style={at={(0.5,-0.35)},anchor=north,legend columns=-1},
            nodes near coords,
            every node near coord/.append style={
                font=\tiny,
                rotate=90,
                anchor=west,
                /pgf/number format/.cd, fixed, precision=2
            }
        ]
        % Enclave Run10_SEQ (10 runs avg)
        \addplot[pattern=north east lines,pattern color=black] coordinates {
            (Elapsed\ Time,24.87)
            (DPS,405000)
            (User\ CPU,41.15)
            (System\ CPU,41.00)
            (Max\ RSS,10)
            (Minor\ PF,1)
            (Major\ PF,0.001)
            (Voluntary\ CS,49)
            (Involuntary\ CS,65992)
        };
        % Enclave Run5* (only 2 valid runs avg)
        \addplot[pattern=dots,pattern color=black] coordinates {
            (Elapsed\ Time,24.67)
            (DPS,405500)
            (User\ CPU,41.15)
            (System\ CPU,41.00)
            (Max\ RSS,10)
            (Minor\ PF,1)
            (Major\ PF,0.001)
            (Voluntary\ CS,49)
            (Involuntary\ CS,65992)
        };
        \legend{Enclave Run10\_SEQ, Enclave Run5*}
        \end{axis}
        \end{tikzpicture}
        \caption{Log-scale comparison of Dhrystone Enclave (KE): Run10\_SEQ vs Run5*.\\
        \textit{* Run5 includes only partial runs (others failed).}}
        \label{fig:dhrystone_ke_runs_comparison}
    \end{subfigure}

    \vspace{1cm} % spacing between subfigures

    % Second Subfigure: Run10_SEQ vs Run5* KE
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            bar width=8pt,
            width=\textwidth,
            height=8cm,
            ymode=log,
            log origin=infty,
            ymin=0.001,
            ylabel={Value (log scale)},
            symbolic x coords={
                Total\ Time,
                Iterations/Sec,
                User\ CPU\ Time,
                System\ CPU\ Time,
                Max\ RSS,
                Minor\ PF,
                Major\ PF,
                Voluntary\ CS,
                Involuntary\ CS
            },
            xtick=data,
            x tick label style={rotate=30, anchor=east},
            enlarge x limits=0.05,
            ymajorgrids,
            grid style=dashed,
            legend style={at={(0.5,-0.35)},anchor=north,legend columns=-1},
            nodes near coords,
            every node near coord/.append style={
                font=\tiny,
                rotate=90,
                anchor=west,
                /pgf/number format/.cd, fixed, precision=2
            }
        ]
        % Run10_SEQ (10 completed runs)
        \addplot[pattern=north east lines,pattern color=black] coordinates {
            (Total\ Time,107.43)
            (Iterations/Sec,929.56)
            (User\ CPU\ Time,41.16)
            (System\ CPU\ Time,41.00)
            (Max\ RSS,10)
            (Minor\ PF,1)
            (Major\ PF,0.001)
            (Voluntary\ CS,49)
            (Involuntary\ CS,65992)
        };
        % Run5* (3/5 completed runs, 2 failed)
        \addplot[pattern=dots,pattern color=black] coordinates {
            (Total\ Time,106.48)
            (Iterations/Sec,939.16)
            (User\ CPU\ Time,41.16)
            (System\ CPU\ Time,41.00)
            (Max\ RSS,10)
            (Minor\ PF,1)
            (Major\ PF,0.001)
            (Voluntary\ CS,49)
            (Involuntary\ CS,65992)
        };
        \legend{Run10\_SEQ, Run5*}
        \end{axis}
        \end{tikzpicture}
        \caption{Log-scale comparison of CoreMark Enclave (KE) performance: Run10\_SEQ vs Run5*.\\
        \textit{* Run5 includes only 3 out of 5 completed runs. Runs 4 and 5 failed.}}
        \label{fig:coremark_ke_runs_comparison}
    \end{subfigure}
\end{figure}


Beyond sequential execution, attempts to scale enclave workloads via parallelism revealed critical hardware-enforced constraints. Specifically, running 10 concurrent parallel enclave instances consistently failed due to exhaustion or conflicts in Physical Memory Protection (PMP) region allocation. This limitation manifested as execution aborts or enclave crashes, highlighting the sensitivity of enclave concurrency to limited hardware memory isolation resources.

Reducing parallelism to 5 simultaneous runs yielded improved but still imperfect stability: intermittent failures occurred less frequently, suggesting that PMP regions remain a bottleneck under moderate concurrency levels. Nevertheless, the 5$\times$ parallel configuration was generally capable of completing most benchmark runs.

In contrast, 2$\times$ parallel enclave runs executed reliably with no PMP-related failures, though at the expected overhead cost relative to native concurrency. Additionally, all 10 sequential enclave runs were stable and completed successfully, underscoring sequential execution as the most dependable operational mode given current PMP constraints.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.75\linewidth]{figures/enclave_parallelism_stability.png}
%   \caption{Effect of parallel enclave instances on execution stability. Failures increase sharply beyond 5 concurrent runs due to PMP region limitations.}
%   \label{fig:parallelism_stability}
% \end{figure}

These results underscore a fundamental trade-off in enclave execution scalability: while parallelism can improve throughput, strict hardware PMP enforcement imposes hard limits on enclave concurrency that manifest as failures at higher parallel counts. Addressing this will likely require advances in PMP management, dynamic region allocation, or hardware extensions to support larger concurrent enclave workloads without compromising memory protection guarantees.

Enclave execution imposes moderate but consistent performance overheads relative to native runs, with cycle counts and execution times increasing approximately 20--60\%. Memory usage remains tightly bounded, and paging is effectively avoided. The most substantial system-level overhead is the marked increase in involuntary context switches during enclave operation, which affects timing predictability but not computational correctness.

In summary, enclave execution on this low-end RISC-V platform introduces moderate but consistent overheads, primarily in the form of increased runtime and context switching, without significant memory bloat or paging pressure. While sequential workloads complete reliably, attempts at concurrent enclave execution expose hard architectural limits rooted in PMP region constraints. These results highlight the current scalability boundaries of enclave systems and motivate the need for both software- and hardware-level support to unlock robust parallelism under secure execution.

\section{Comparative Overhead Metrics Across Workloads}
\label{sec:workload-sensitivity}

This section presents a comparative evaluation of the performance overheads introduced by secure enclave execution across three representative workloads: the Kyber post-quantum cryptographic scheme, the Dhrystone integer benchmark, and the CoreMark embedded systems benchmark. These workloads were selected for their diversity in computational characteristics—ranging from cryptography-heavy operations with irregular memory access patterns, to synthetic instruction-driven integer arithmetic, and finally to mixed workload execution involving control logic, arithmetic, and memory access. By comparing enclave and native performance across these distinct benchmarks, we aim to empirically characterize how enclave runtime behavior and hardware-level protections interact with different application classes.

All experiments were conducted on the same low-end RISC-V platform: a single in-order core with 64~MB of RAM and no hardware acceleration for encryption or virtualization. Both enclave and native executions used the same compiler flags, runtime configurations, and benchmarking scripts. Each benchmark was run ten times sequentially in both native and enclave modes to minimize measurement noise and provide stable median and average performance statistics. Measurements included CPU cycle counts, wall-clock time, throughput (where applicable), context switches, and memory footprint.

Among the workloads tested, Kyber was the most sensitive to enclave-related overhead. In native execution, the \texttt{kyber\_decaps} operation completed in a median of approximately 2.9 million cycles, corresponding to a wall-clock time of around 10.7 seconds. However, under enclave execution, the same operation required over 3.5 million cycles and a wall-clock time of 29.2 seconds, reflecting a 2.73$\times$ increase in observed duration. The increase in cycle count—approximately 20\%—was dwarfed by the nearly 3$\times$ increase in elapsed time, suggesting that the overhead is not merely computational but also systemic. The Kyber implementation is particularly sensitive to memory access latency due to its reliance on large data buffers, frequent pointer dereferencing, and memory-intensive operations such as polynomial multiplication and compression. These characteristics make it acutely vulnerable to the latency overhead introduced by enclave-specific features such as memory encryption and integrity verification, which intercept and transform nearly every memory read and write.

By contrast, CoreMark exhibited far greater resilience to enclave-induced slowdown. The native execution time averaged 101.5 seconds, while enclave execution required 107.3 seconds—a modest 1.06$\times$ slowdown. Throughput dropped slightly from a native range of 967--988 iterations per second to an enclave range of 919--939, indicating that while performance was affected, the impact was minimal relative to the workload's duration and structure. This result is consistent with CoreMark’s design as a balanced benchmark that includes control flow, integer operations, and limited memory activity. The longer total runtime may also help amortize fixed enclave setup and teardown costs over a larger instruction volume, dampening the perceived overhead.

Figure~\ref{fig:sensitivity-overheads-chart} summarizes the comparative performance and system-level behavior across all three benchmarks, illustrating the wall-clock time overheads and the uniformly high context switch counts observed during enclave execution.

Dhrystone, a classic synthetic benchmark designed to test CPU and compiler performance through tight integer loops and frequent procedure calls, showed intermediate sensitivity. Its native execution time averaged 15.3 seconds, while the enclave variant took 24.7 seconds—an overhead factor of approximately 1.61$\times$. Unlike Kyber, Dhrystone does not perform extensive memory operations, but its compact and time-sensitive control loops make it vulnerable to any systemic delay introduced by the enclave runtime, particularly if context switches or instruction fetch penalties are frequent. Its throughput dropped slightly, from around 400,000 operations per second natively to a range between 395,000 and 408,000 in the enclave, with the decline being small but consistent.

Across all three benchmarks, a consistent and striking overhead pattern emerged in the form of involuntary context switches. While native executions averaged fewer than 400 involuntary context switches per run, enclave executions across all workloads exhibited approximately 66,000 context switches per run—an increase of nearly two orders of magnitude. This effect was uniform across workloads, suggesting that it is driven by enclave runtime behavior rather than application-specific activity. Likely contributors include timer interrupts, enclave runtime transitions, secure fault handling, and scheduling-related activity under the Keystone framework. Despite this significant increase, the impact on user-mode computation was minimal in terms of CPU time distribution: more than 95\% of execution time was consistently spent in user space, indicating that the workload remained compute-bound and that the primary cost of context switching was time variability rather than computational interference.

Memory usage was consistent and tightly constrained across all runs. All workloads exhibited a resident set size (RSS) of approximately 10~KB in both enclave and native executions, with no major page faults and typically only a single minor page fault per run. This indicates that working sets for each benchmark fit entirely within the secure memory allocation provided by the enclave runtime, and that the paging subsystem—despite operating in a protected mode—remained effective and unobtrusive. This stability is particularly noteworthy given the limited RAM available and the additional overhead of enclave memory isolation mechanisms.

Taken together, the data suggests that the performance impact of enclave execution is highly dependent on the nature of the workload. Kyber, with its pointer-heavy, memory-intensive cryptographic routines, suffers the most under enclave constraints, particularly due to the latency introduced by secure memory mechanisms. Dhrystone, while less affected, still shows measurable sensitivity due to its reliance on fast, loop-driven control structures. CoreMark, in contrast, demonstrates a relatively minor slowdown, benefiting from workload diversity and longer runtime that dilutes per-access costs. At the system level, enclave-induced context switching is a universal phenomenon, independent of workload type, but it does not appear to disrupt user-mode execution beyond timing variability.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}

% --- Left axis: Execution times ---
\begin{axis}[
    ybar,
    bar width=6pt,
    width=14cm,
    height=8cm,
    ymin=0,
    ymax=120,
    ylabel={Execution Time (s)},
    symbolic x coords={Kyber,Dhrystone,CoreMark},
    xtick=data,
    enlarge x limits=0.4,
    ymajorgrids,
    grid style=dashed,
    legend style={at={(0.5,-0.18)},anchor=north,legend columns=3},
    axis y line*=left,
    axis x line*=bottom,
    bar shift=-4pt
]
% Native time
\addplot[pattern=dots,pattern color=black] 
    coordinates {(Kyber,10.7) (Dhrystone,15.3) (CoreMark,101.5)};

% Enclave time + overhead factor labels
\addplot[
    pattern=dots,
    pattern color=black!50,
    nodes near coords,
    point meta=explicit symbolic,
    every node near coord/.append style={
        font=\small,
        anchor=south,
        yshift=2pt,
        fill=white,
        fill opacity=0.7,
        text opacity=1
    }
] 
coordinates {
    (Kyber,29.2)      [2.73$\times$]
    (Dhrystone,24.7)  [1.61$\times$]
    (CoreMark,107.3)  [1.06$\times$]
};

\legend{Native Time, Enclave Time}
\end{axis}

% --- Right axis: Context switches ---
\begin{axis}[
    ybar,
    bar width=6pt,
    width=14cm,
    height=8cm,
    ymin=0,
    ymax=70000,
    ylabel={Involuntary Context Switches},
    symbolic x coords={Kyber,Dhrystone,CoreMark},
    xtick=data,
    enlarge x limits=0.4,
    hide x axis,
    axis y line*=right,
    axis x line=none,
    bar shift=10pt
]
\addplot[pattern=north east lines,pattern color=black] 
    coordinates {(Kyber,66000) (Dhrystone,66000) (CoreMark,66000)};
\legend{Involuntary CS}
\end{axis}

\end{tikzpicture}
\caption{Comparison of native and enclave execution times (left axis) with slowdown factors annotated, and involuntary context switch counts (right axis).}
\label{fig:sensitivity-overheads-chart}
\end{figure}

Overall, the impact of enclave execution overheads varies sharply by workload class, with memory-intensive cryptographic routines like Kyber suffering the most, and compute-dominant tasks like CoreMark showing minimal disruption. Importantly, system-level costs such as context switching appear invariant to workload structure, suggesting that certain overheads are intrinsic to enclave runtime design rather than application behavior. These distinctions are critical for guiding future secure system optimizations, especially in embedded or resource-constrained environments.    

Among the evaluated workloads, \texttt{coremark} stood out as the only one to exhibit consistent, positive scaling behavior. Performance improved by approximately 40.8\% with each additional core, and a similarly proportional uplift was observed with increased memory. This result is aligned with earlier observations: CoreMark’s mixed-instruction profile and parallelism-friendly design allow it to exploit both increased computational throughput and expanded memory capacity. As a compute-bound benchmark with moderate data locality, CoreMark scales predictably and efficiently with additional hardware provisioning.

In contrast, other workloads revealed far less favorable scaling characteristics. \texttt{Dhrystone}, for example, degraded by an average of 27.3\% per increment in either core or memory capacity. This counterintuitive result likely stems from architectural inefficiencies introduced by scaling: increased cache contention, ineffective parallel scheduling, or synchronization overheads may all contribute to this decline. Given Dhrystone’s tightly looped, serial control flow, its structure inherently resists parallelization and may suffer under hardware configurations that introduce unnecessary execution complexity.

Similarly, the \texttt{kyber} benchmark exhibited performance degradation averaging 26.1\% with increased cores and memory. These declines reflect Kyber’s sensitivity to memory hierarchy disruptions and suggest that parallelism may exacerbate contention for shared resources or increase memory access latency in unpredictable ways. Given Kyber’s cryptographic design, which involves frequent pointer dereferencing and buffer manipulation, increased hardware does not necessarily translate into improved performance and may instead magnify system-level inefficiencies.

For enclave-protected variants—\texttt{coremark.ke}, \texttt{enclave.ke}, and \texttt{kyber.ke}—the effect of hardware scaling was largely negligible. Across all test configurations, performance remained flat, indicating that these workloads are fundamentally bottlenecked by enclave runtime constraints rather than available compute or memory resources. Single-threaded enclave runtimes, fixed enclave memory regions, and serialization of cryptographic protections (e.g., memory encryption, page integrity) all limit the extent to which additional hardware can be utilized. These behaviors reinforce the architectural ceiling imposed by current enclave frameworks, especially under Physical Memory Protection (PMP) constraints observed earlier.


\section{Performance Sensitivity to Core and Memory Scaling}
\label{sec:hardware-impact}

Building on the preceding workload-specific performance analysis, we now examine how the availability of additional hardware resources—specifically CPU cores and physical memory—affects execution performance across native and enclave workloads on our constrained RISC-V platform. In conventional computing environments, such scaling is often assumed to yield linear or super-linear performance gains. However, embedded systems with secure enclaves (TEEs) operate under very different architectural and runtime constraints. Synchronization overheads, limited memory bandwidth, context-switching penalties, and enclave entry/exit costs all contribute to the complex performance dynamics of these systems.

To investigate this, we performed a systematic evaluation of six representative workloads spanning both native and secure execution modes: \texttt{coremark}, \texttt{coremark.ke}, \texttt{dhrystone}, \texttt{enclave.ke}, \texttt{kyber}, and \texttt{kyber.ke}. Each workload was benchmarked across multiple hardware configurations, incrementally increasing the number of CPU cores and the amount of available physical memory. Performance was quantified as relative throughput (operations per second), normalized across scaling steps to isolate the impact of hardware changes.

Table~\ref{tab:hardware-impact} presents a high-level summary of the results. For each scaling axis (cores and memory), we classify workloads based on their observed performance response—whether they improved, degraded, or remained effectively unchanged. In addition, we report the average performance change per scaling increment, aggregated across all workloads.

\begin{table}[htbp]
\centering
\begin{threeparttable}
\caption{Summary of workload responses to incremental hardware scaling. Negative average values indicate improved performance (lower runtime) per scaling step.}
\label{tab:hardware-impact}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{CPU cores} & \textbf{Memory (MB)} \\
\midrule
Workloads improved                      & 2/6 (33\%) & 2/6 (33\%) \\
Workloads slowed down                   & 2/6 (33\%) & 2/6 (33\%) \\
Workloads unaffected                    & 2/6 (33\%) & 2/6 (33\%) \\
Average performance change per step (\%)& -8.71      & -8.71      \\
Median performance change per core step (\%) & -0.51      & -0.51      \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\footnotesize
\item Each increment corresponds to one unit increase in either CPU cores or available memory.
\item Values reflect direction and magnitude of average runtime change across all workloads.
\end{tablenotes}
\end{threeparttable}
\end{table}

The results reveal a highly heterogeneous response to hardware scaling. Out of the six workloads, two demonstrated consistent performance improvements with additional resources, two exhibited measurable performance degradation, and two were largely insensitive to the changes. This one-third distribution across outcome categories—improved, degraded, and unaffected—highlights a key finding: hardware scaling in secure embedded platforms does not universally translate into improved performance. In many cases, the architectural and software stack limitations may nullify or even counteract the expected benefits of scaling.

A closer inspection of individual trends provides additional insight. Workloads such as \texttt{coremark.ke} and \texttt{dhrystone.ke} showed modest but consistent improvements as hardware resources increased, with average runtime reductions of approximately $1.7\%$ and $2.6\%$ per step, respectively. These results suggest that, under certain execution conditions—some enclave workloads can benefit from added parallelism or memory availability.

In contrast, the native \texttt{dhrystone} and \texttt{kyber} workloads exhibited significant performance degradation, with average runtime increases exceeding $35\%$ per scaling increment. This counterintuitive behavior may stem from increased scheduling overheads, memory contention, or inefficiencies introduced by the runtime environment when additional cores or memory are made available without corresponding application-level parallelism or optimization. These findings underscore the risk of assuming that more resources automatically lead to better performance—particularly in non-threaded or memory-bound scenarios.

Meanwhile, workloads such as \texttt{coremark} and \texttt{kyber.ke} appeared largely indifferent to hardware scaling. Their performance remained flat across all configurations, suggesting that their bottlenecks lie elsewhere—possibly in instruction dispatch, I/O constraints, or TEE-specific execution boundaries that do not benefit from resource scaling.

Statistically, the overall average performance change per scaling step was a negative $-8.71\%$, with a median change of just $-0.51\%$. This gap between the mean and median indicates a skewed distribution, driven by a few strongly negative outliers (notably \texttt{kyber} and \texttt{dhrystone}) rather than uniform underperformance across all workloads.

To rigorously assess the significance of these observed performance trends, we applied both parametric and non-parametric statistical tests across all configurations. Specifically, a one-way ANOVA test was used to evaluate mean differences between scaling levels (under the assumption of normality), while the Kruskal–Wallis test assessed distributional differences in a non-parametric manner. These results are visualized in Figure~\ref{fig:combined-analysis-updated}.

%\footnote{The Kruskal-Wallis test was used alongside ANOVA to handle cases where group distributions violated normality assumptions, ensuring robustness in detecting group differences. Random Forest regression was applied in addition to linear models to address multicollinearity between hardware parameters (e.g., cores and memory), and to capture potential nonlinear relationships that linear regression may miss.}

\begin{figure}[htbp]
\centering

% --- Subfigure (a): p-value Bar Chart ---
\begin{subfigure}[t]{0.95\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\linewidth,
    height=7cm,
    ybar,
    bar width=10pt,
    ymin=0, ymax=1,
    ymajorgrids,
    grid style=dashed,
    symbolic x coords={coremark, coremark.ke, dhrystone, dhrystone.ke, kyber, kyber.ke},
    xtick=data,
    xticklabel style={font=\small, rotate=30, anchor=east},
    enlarge x limits=0.15,
    ylabel={p-value},
    axis y line*=left,
    axis x line*=bottom,
    font=\small,
    legend style={font=\small, at={(0.5,1.1)}, anchor=south, legend columns=2},
    nodes near coords,
    every node near coord/.append style={
        font=\scriptsize,
        rotate=90,
        anchor=west,
        yshift=2pt
    }
]

% ANOVA
\addplot+[ybar, pattern=crosshatch, pattern color=black, draw=black, bar shift=-6pt] coordinates {
  (coremark, 0.827) (coremark.ke, 0.113) (dhrystone, 0.193)
  (dhrystone.ke, 0.353) (kyber, 0.138) (kyber.ke, 0.421)
};

% Kruskal-Wallis
\addplot+[ybar, pattern=north east lines, pattern color=black, draw=black, bar shift=6pt] coordinates {
  (coremark, 0.967) (coremark.ke, 0.417) (dhrystone, 0.306)
  (dhrystone.ke, 0.160) (kyber, 0.258) (kyber.ke, 0.865)
};

\legend{ANOVA p-values, Kruskal-Wallis p-values}

% Significance line at p = 0.05
\draw[black, dashed] (rel axis cs:0,0.05) -- (rel axis cs:1,0.05);

\end{axis}
\end{tikzpicture}
\caption{ANOVA and Kruskal-Wallis p-values for core and memory scaling. p-values below the $\alpha=0.05$ threshold (dashed line) indicate statistically significant effects.}
\label{fig:anova-kruskal}
\vspace{0.5em}
{\footnotesize
\noindent\parbox{\linewidth}{%
\textit{Note: The Kruskal-Wallis test was included to address cases where normality assumptions were not met.}
}
}
\end{subfigure}

\vspace{1.5em}

% --- Subfigure (b): Grouped R^2 Chart ---
\begin{subfigure}[t]{0.95\linewidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\linewidth,
    height=7cm,
    ybar,
    bar width=10pt,
    enlarge x limits=0.15,
    ylabel={$R^2$ (interaction model)},
    ymin=0, ymax=0.5,
    ymajorgrids,
    grid style=dashed,
    symbolic x coords={coremark,coremark.ke,dhrystone,dhrystone.ke,kyber,kyber.ke},
    xtick=data,
    xticklabel style={rotate=30,anchor=east, font=\small},
    font=\small,
    legend style={at={(0.5,1.1)}, anchor=south, legend columns=2},
    every node near coord/.append style={
        font=\footnotesize,
        rotate=90,
        anchor=west,
        /pgf/number format/fixed,
        /pgf/number format/precision=2
    },
    nodes near coords
]

% Linear regression
\addplot+[
    pattern=north east lines, 
    pattern color=black, 
    draw=black, 
    bar shift=-6pt
] coordinates {
    (coremark,0.04)
    (coremark.ke,0.07)
    (dhrystone,0.28)
    (dhrystone.ke,0.04)
    (kyber,0.33)
    (kyber.ke,0.24)
};

% Random Forest
\addplot+[
    pattern=horizontal lines, 
    pattern color=black, 
    draw=black, 
    bar shift=6pt
] coordinates {
    (coremark,0.07)
    (coremark.ke,0.40)
    (dhrystone,0.31)
    (dhrystone.ke,0.24)
    (kyber,0.36)
    (kyber.ke,0.23)
};

\legend{Linear regression, Random Forest}
\end{axis}
\end{tikzpicture}
\caption{Comparison of $R^2$ values from linear regression and random forest models across workloads. Higher values indicate better model fit.}
\label{fig:r2_grouped}
\vspace{0.5em}
{\footnotesize
\noindent\parbox{\linewidth}{%
\textit{Note: Random Forest regression was included to address multicollinearity issues observed in linear models.}
}
}
\end{subfigure}

\caption{Statistical analysis of workload performance scaling. (a) ANOVA and Kruskal-Wallis tests evaluate whether core/memory scaling significantly affects performance. (b) Regression models estimate the effect magnitude and fit quality.}
%\caption*{\textit{Note: A Random Forest regressor was used to mitigate multicollinearity effects between CPU cores and memory capacity, offering a non-linear alternative to standard linear regression.}}
\label{fig:combined-analysis-updated}
\end{figure}

As shown in Figure~\ref{fig:anova-kruskal}, none of the $p$-values from either ANOVA or Kruskal--Wallis testing fell below the $\alpha = 0.05$ significance threshold. This indicates a lack of statistically significant performance variation across hardware scaling steps for all workloads tested. The $R^2$ values in subfigure~\ref{fig:r2_grouped} support this, with low model fit observed in both linear and Random Forest regressions, suggesting weak or no explanatory power.

%In summary, while certain workloads demonstrate mild sensitivity to hardware scaling, these effects are neither statistically significant nor consistent across the benchmark suite. This finding has important implications for the design of secure embedded systems: namely, that simply increasing hardware capabilities—whether by adding cores or memory—does not guarantee improved performance under TEE constraints. Instead, performance tuning must consider software-level optimizations, workload characteristics, and the inherent cost model of enclave-based execution.
