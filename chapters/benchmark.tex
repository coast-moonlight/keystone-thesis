\chapter{Benchmarking Results and Observations}
\label{chap:benchmarking}

This chapter presents a detailed empirical evaluation of the Keystone enclave framework, with a specific focus on its performance characteristics under a range of representative workloads. The primary aim is to quantify the computational and system-level overheads introduced by secure enclave execution on RISC-V platforms, and to explore how these overheads are influenced by underlying hardware configurations and architectural constraints. By systematically comparing enclave-based execution against native (non-isolated) execution, we aim to identify the key trade-offs and limitations that developers and system architects must consider when adopting enclaves for trusted execution.

To achieve this, we deploy a carefully selected set of benchmark workloads that span both general-purpose and security-critical application domains. These include the Dhrystone and CoreMark benchmarks—commonly used to assess CPU performance and embedded system efficiency—as well as the Kyber post-quantum cryptographic algorithm, which provides a real-world workload with stringent security and computational requirements. Together, these benchmarks provide a diverse and comprehensive view of how Keystone enclaves behave across different execution contexts.

The benchmarking process is structured to examine four interrelated dimensions. First, we establish a native performance baseline to isolate the impact of enclave-related operations. Second, we evaluate the performance of the same workloads when executed within enclaves, highlighting the latency and resource costs incurred by isolation mechanisms such as Physical Memory Protection (PMP) and context switching. Third, we perform a workload sensitivity analysis, comparing the behavior of different benchmark types—sequential vs. parallel, lightweight vs. compute-intensive—to determine how workload characteristics affect enclave performance. Finally, we assess the influence of key hardware parameters, including CPU core count, memory size, and cache configuration, on both native and enclave execution.

Through these experiments, we uncover both expected and non-trivial behaviors. For instance, while enclave overheads are generally modest—typically under 15\%—they are highly sensitive to parallelism and memory access patterns. Furthermore, we identify critical bottlenecks, such as the fixed number of PMP entries available for isolating memory regions, which directly limit enclave concurrency and scalability.

The results presented in this chapter serve as the empirical foundation for the interpretive analysis in the next chapter. By rigorously characterizing the performance profile of Keystone enclaves, we establish a data-driven basis for understanding the broader implications of enclave deployment in secure system design, as well as informing practical strategies for optimization and resource management.

\section{Baseline: Native (Non-Enclave) Performance}
\label{sec:baseline-native}

To accurately assess the performance overheads imposed by secure enclave execution, it is imperative to begin with a rigorously characterized baseline: one that isolates application-level behavior from security mechanisms, scheduling anomalies, and hardware acceleration artifacts. This section documents such a baseline, derived from native (non-enclave) execution of representative workloads on a deterministic, single-core RISC-V simulation environment. All benchmarks were executed under the Low-End Baseline configuration, designed to approximate the resource constraints of a lightweight embedded platform: 64~MB of main memory, a single in-order RISC-V core, and no parallelism beyond sequential instruction issue. This environment intentionally excludes vector extensions, out-of-order speculation, and memory prefetching, ensuring that measured performance reflects algorithmic structure rather than microarchitectural opportunism.

Each benchmark was executed both \emph{sequentially} and \emph{in parallel}, ten times each, and the resulting figures compare performance and stability across both modes.

% ------------------------
% Wall-clock time figures
% ------------------------
Figure~\ref{fig:wallclock_coremark} reports CoreMark's performance profile using the latest data from Run~10 of both sequential and parallel execution modes. Execution time remained consistent, with the sequential case averaging approximately 101.17 seconds of wall-clock time and the parallel case averaging around 103.37 seconds. This corresponds to an effective throughput of about 988.4 iterations per second for the sequential run and 967.4 iterations per second for the parallel run. User-mode CPU time dominated in both cases (101.20 seconds sequential, 103.39 seconds parallel), accounting for over 99.9\% of total CPU time. System-mode CPU time was negligible (under 0.03 seconds), confirming that the workload’s operational footprint is almost entirely in user space with minimal reliance on system calls or kernel transitions.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/wallclock_coremark.png}
%   \caption{CoreMark wall-clock time in native sequential and parallel runs.}
%   \label{fig:wallclock_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/wallclock_dhrystone.png}
%   \caption{Dhrystone wall-clock time in native sequential and parallel runs.}
%   \label{fig:wallclock_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/wallclock_kyber.png}
%   \caption{Kyber wall-clock time in native sequential and parallel runs.}
%   \label{fig:wallclock_kyber}
% \end{figure}

% ------------------------
% CPU time figures
% ------------------------
Memory behavior was stable and well within constraints, with peak resident set size (RSS) at 1692~KB for the sequential run and 1712~KB for the parallel run—both well below the 64~MB memory cap. This indicates that the benchmark’s data and code footprint remained fully cacheable, without triggering paging or dynamic memory expansion. Minor page faults were low and consistent (82--84 for sequential, 82--85 for parallel), though a single major page fault was observed in the parallel run and two in the sequential run, suggesting rare but possible demand paging during lazy allocation. Voluntary context switches were minimal (0--1 sequential, 0--3 parallel), while involuntary context switches showed a stark contrast: approximately 350 in the sequential case versus over 25,800 in the parallel case. This disparity reflects a significantly higher degree of preemption or timer interrupts in the parallel workload environment.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cputime_coremark.png}
%   \caption{CoreMark CPU time (user and system) in native sequential and parallel runs.}
%   \label{fig:cputime_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cputime_dhrystone.png}
%   \caption{Dhrystone CPU time (user and system) in native sequential and parallel runs.}
%   \label{fig:cputime_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cputime_kyber.png}
%   \caption{Kyber CPU time (user and system) in native sequential and parallel runs.}
%   \label{fig:cputime_kyber}
% \end{figure}

% ------------------------
% Throughput figures
% ------------------------
Figure~\ref{fig:dhrystone_native} presents detailed results from the Dhrystone benchmark under native conditions. The sequential runs exhibited an average elapsed time of approximately 101.2 seconds per run, with user-mode CPU time closely matching this figure, indicating near-total CPU utilization. In contrast, the parallel merged runs completed in roughly 103.4 seconds, with user CPU time again nearly equal to wall-clock time. Throughput, measured in Dhrystone operations per second (DPS), was higher in the sequential case—approximately 988.4~DPS versus 967.4~DPS in the parallel configuration—reflecting slight scheduling overhead from virtual concurrency under QEMU.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/throughput_coremark.png}
%   \caption{CoreMark throughput in native sequential and parallel runs.}
%   \label{fig:throughput_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/throughput_dhrystone.png}
%   \caption{Dhrystone throughput in native sequential and parallel runs.}
%   \label{fig:throughput_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/throughput_kyber.png}
%   \caption{Kyber throughput in native sequential and parallel runs.}
%   \label{fig:throughput_kyber}
% \end{figure}

% ------------------------
% Memory usage figures
% ------------------------
Memory footprint remained small (peak RSS ~1.7~MB) and stable, with 82--86 minor page faults per run.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/memory_rss_coremark.png}
%   \caption{CoreMark resident set size (RSS) in native sequential and parallel runs.}
%   \label{fig:memory_rss_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/memory_rss_dhrystone.png}
%   \caption{Dhrystone resident set size (RSS) in native sequential and parallel runs.}
%   \label{fig:memory_rss_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/memory_rss_kyber.png}
%   \caption{Kyber resident set size (RSS) in native sequential and parallel runs.}
%   \label{fig:memory_rss_kyber}
% \end{figure}

% ------------------------
% Page faults figures
% ------------------------
% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/pagefaults_coremark.png}
%   \caption{CoreMark minor and major page faults in native sequential and parallel runs.}
%   \label{fig:pagefaults_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/pagefaults_dhrystone.png}
%   \caption{Dhrystone minor and major page faults in native sequential and parallel runs.}
%   \label{fig:pagefaults_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/pagefaults_kyber.png}
%   \caption{Kyber minor and major page faults in native sequential and parallel runs.}
%   \label{fig:pagefaults_kyber}
% \end{figure}

% ------------------------
% Context switches figures
% ------------------------
Voluntary context switches were minimal (0--1 sequential, 0--3 parallel), while involuntary context switches showed a stark contrast: approximately 350 in the sequential case versus over 25,800 in the parallel case.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/contextswitches_coremark.png}
%   \caption{CoreMark voluntary and involuntary context switches in native sequential and parallel runs.}
%   \label{fig:contextswitches_coremark}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/contextswitches_dhrystone.png}
%   \caption{Dhrystone voluntary and involuntary context switches in native sequential and parallel runs.}
%   \label{fig:contextswitches_dhrystone}
% \end{figure}

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/contextswitches_kyber.png}
%   \caption{Kyber voluntary and involuntary context switches in native sequential and parallel runs.}
%   \label{fig:contextswitches_kyber}
% \end{figure}

% ------------------------
% Cycle counts figures (Kyber only)
% ------------------------
Kyber performance was evaluated using the official \texttt{test\_speed512} binary from the reference implementation's \texttt{ref/} directory.\footnote{See \url{https://github.com/pq-crystals/kyber}. The \texttt{test\_speed512} utility executes keypair generation, encapsulation, and decapsulation 1000 times each and reports cycle counts using architecture-level counters emulated under QEMU.} Table~\ref{tab:kyber_run10_cycles} summarizes the median cycle counts for each operation, while Figure~\ref{fig:kyber_native_cycles} compares sequential and parallel execution.

% \begin{figure}[htbp]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/cyclecounts_kyber.png}
%   \caption{Kyber median cycle counts for keypair generation, encapsulation, and decapsulation in native sequential and parallel runs.}
%   \label{fig:kyber_native_cycles}
% \end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/kyber_64M_1Core.pdf}
    \caption{Comparison of Kyber Median CPU Cycles per Operation: Sequential Single-Threaded (\texttt{run10\_SEQ}) vs Parallel Merged (\texttt{run10}) Execution (10 Runs)}
    \label{fig:kyber_native_cycles}
\end{figure}
All cryptographic operations executed entirely in user space, without invoking privileged instructions or incurring system-level overhead. Memory footprint remained minimal (peak RSS ~10~KB), with only a single minor page fault observed across all runs.

\begin{table}[H]
\centering
\scriptsize
\caption{Median CPU Cycles for Kyber Operations}
\begin{tabular}{|l|r|r|}
\hline
\textbf{Operation} & \textbf{Parallel Run10 Median (cycles)} & \textbf{Sequential Run10 Median (cycles)} \\
\hline
indcpa\_keypair & 1,870,200 & 1,702,800 \\
\hline
indcpa\_enc     & 38,376,800 & 2,174,400 \\
\hline
indcpa\_dec     & 39,060,800 & 618,400 \\
\hline
kyber\_keypair  & 1,870,200 & 1,800,300 \\
\hline
kyber\_encaps   & 38,376,800 & 2,285,900 \\
\hline
kyber\_decaps   & 39,060,800 & 2,921,800 \\
\hline
\end{tabular}
\label{tab:kyber_median_cycles}
\end{table}

Across all three workloads, cache behavior was uniformly stable. Monitoring via QEMU’s cache plugin confirmed low instruction and data cache miss rates, consistent with compact memory footprints and tight loop-local access patterns. Instruction cache performance benefited from small binary sizes and the absence of dynamic control flow, while data cache pressure remained minimal due to statically sized inputs and short-lived stack allocations. These characteristics consistently indicate compute-bound, not memory-bound, behavior in the native case.

Together, these results confirm that the native execution environment offers a low-noise, deterministic baseline across benchmarks. Variability was minimal, CPU time aligned closely with wall-clock time, memory remained well-bounded, and system-level interference was negligible—making this configuration ideal for isolating performance effects introduced in subsequent secure or enclave-based execution modes.

\section{Enclave Execution Results and Overheads}
\label{sec:enclave-execution}

Building on the native baseline characterization, this section analyzes the performance impacts and system overheads observed when executing representative workloads—Kyber, Dhrystone, and CoreMark—within a secure enclave environment. The experiments were conducted on the identical Low-End Baseline platform (64~MB RAM, single in-order RISC-V core), isolating overheads introduced exclusively by enclave security mechanisms such as memory encryption, integrity verification, and enclave runtime management.

All benchmarks were initially executed ten times sequentially under enclave protection to establish a direct, low-variability comparison against native runs. The Kyber \texttt{kyber\_decaps} operation exhibited median CPU cycle counts consistently around 3.5 million cycles per run (e.g., 3,505,686 cycles in Run~1), approximately 1.2$\times$ the native sequential baseline of about 2.9 million cycles (see Table~\ref{tab:kyber_median_cycles}). Average cycles were slightly higher, around 3.53 million, reflecting stable and repeatable enclave performance. Other key Kyber operations, such as \texttt{indcpa\_keypair} and \texttt{indcpa\_enc}, showed median cycle counts near 1.74 million and 2.62 million cycles respectively, consistent across runs.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/kyber_64M_1Core_comparison.pdf}
    \caption{Cycle count comparison for Kyber operations (native vs enclave). Error bars reflect variation over 10 runs.}
    \label{fig:kyber_cycles}
\end{figure}

Corresponding wall-clock times for these Kyber enclave runs averaged approximately 29.3 seconds, significantly exceeding native baselines of 10--11 seconds, primarily due to cryptographic memory encryption and integrity checks that increase the cost of each memory access and instruction fetch.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.7\textwidth]{figures/wall_clock_comparison.pdf}
%    \caption{Wall-clock execution time across benchmarks (native vs enclave, averaged over 10 runs).}
%    \label{fig:wall_clock}
%\end{figure}

Memory footprint across enclave runs remained tightly constrained near 10~KB RSS, mirroring native conditions. Page faults were minimal, with typically one minor page fault and zero major faults per run, indicating effective enclave memory management without paging overhead.

A notable system-level overhead manifested in the form of a dramatic increase in involuntary context switches—approximately 66,000 per run, nearly two orders of magnitude above the roughly 350 switches observed natively. This surge likely reflects enclave runtime activities such as page re-encryption cycles, secure interrupt handling, and frequent enclave scheduler invocations. While these involuntary context switches contribute to wall-clock time variability, user-mode CPU time remained dominant (over 95\% of total CPU time), confirming that computation remains primarily enclave-resident and compute-bound.

The Dhrystone benchmark demonstrated a similar pattern, with wall-clock times averaging 24.7 seconds under enclave protection, roughly 1.6$\times$ slower than the native 15.3-second average. User and system CPU times both rose to near 41 seconds, reflecting the combined cost of enclave runtime overheads and cryptographic memory protection. Throughput, measured in Dhrystone operations per second, declined modestly to the 395,000--408,000 DPS range from a native baseline near 400,000 DPS. Memory consumption and page fault rates stayed consistent with Kyber’s enclave runs, and involuntary context switch counts remained elevated near 66,000 per run.

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=0.7\textwidth]{figures/throughput_comparison.pdf}
%    \caption{Benchmark throughput under native and enclave execution}
%    \label{fig:throughput}
%\end{figure}

CoreMark enclave runs averaged 107 seconds wall-clock time—about 1.05$\times$ slower than the 101-second native average—with throughput dropping to 919--939 iterations per second compared to the native 967--988 range. CPU time profiles and memory usage mirrored the trends seen in Kyber and Dhrystone, reinforcing a stable but nontrivial enclave overhead footprint.

Beyond sequential execution, attempts to scale enclave workloads via parallelism revealed critical hardware-enforced constraints. Specifically, running 10 concurrent parallel enclave instances consistently failed due to exhaustion or conflicts in Physical Memory Protection (PMP) region allocation. This limitation manifested as execution aborts or enclave crashes, highlighting the sensitivity of enclave concurrency to limited hardware memory isolation resources.

Reducing parallelism to 5 simultaneous runs yielded improved but still imperfect stability: intermittent failures occurred less frequently, suggesting that PMP regions remain a bottleneck under moderate concurrency levels. Nevertheless, the 5$\times$ parallel configuration was generally capable of completing most benchmark runs.

In contrast, 2$\times$ parallel enclave runs executed reliably with no PMP-related failures, though at the expected overhead cost relative to native concurrency. Additionally, all 10 sequential enclave runs were stable and completed successfully, underscoring sequential execution as the most dependable operational mode given current PMP constraints.

These results underscore a fundamental trade-off in enclave execution scalability: while parallelism can improve throughput, strict hardware PMP enforcement imposes hard limits on enclave concurrency that manifest as failures at higher parallel counts. Addressing this will likely require advances in PMP management, dynamic region allocation, or hardware extensions to support larger concurrent enclave workloads without compromising memory protection guarantees.

Enclave execution imposes moderate but consistent performance overheads relative to native runs, with cycle counts and execution times increasing approximately 20--60\%. Memory usage remains tightly bounded, and paging is effectively avoided. The most substantial system-level overhead is the marked increase in involuntary context switches during enclave operation, which affects timing predictability but not computational correctness.

These findings confirm that secure enclaves can sustain a wide range of computational workloads with predictable overheads and stable memory behavior, while also revealing important hardware-imposed concurrency limitations. The detailed performance and system metric breakdown presented here forms a critical foundation for future optimization and hardware enhancement efforts targeting secure enclave scalability and efficiency.

\section{Workload Sensitivity Analysis (Kyber vs CoreMark vs Dhrystone)}

The Kyber post-quantum cryptographic algorithm is a representative workload for evaluating the performance impact of enclave execution on security-critical applications. Kyber consists of three primary operations—key generation, encapsulation, and decapsulation—each profiled extensively within enclaves using the Keystone timer API to capture precise cycle counts.

Execution within enclaves increased Kyber’s runtime by approximately 7–9\% compared to native execution. This overhead is modest given the cryptographic operations’ complexity and the security benefits provided by enclaves.

As with native execution, Kyber performance was insensitive to increases in CPU core count due to its inherently sequential design. Memory usage during enclave execution remained stable, with no indications of paging or contention. System-wide cache miss rates observed through QEMU remained consistent with Kyber’s memory access patterns, including phases dominated by number-theoretic transforms and polynomial arithmetic.

From a resource perspective, the requirement of one PMP entry per enclave constrains the number of parallel Kyber instances that can be simultaneously deployed. This constraint must be taken into account when designing systems requiring multiple isolated cryptographic operations.

\section{Impact of Hardware Parameters (Cores, Memory, Cache)}

To comprehensively assess how hardware configurations affect enclave performance, we systematically varied CPU core counts, memory allocations, and cache configurations.

Scaling the number of CPU cores yielded anticipated improvements for parallel workloads, with throughput gains plateauing beyond six cores due to synchronization overhead and increased enclave management complexity. Sequential workloads such as Kyber remained unaffected by core scaling.

Memory allocation experiments revealed that increasing memory size from 64MB to 2GB improved performance for memory-intensive benchmarks by reducing paging and increasing cache hit rates. However, beyond approximately 512MB, gains diminished, indicating sufficient memory provision for the workload’s working set.

Cache configuration impacts were evaluated using the QEMU cache modeling plugin. Larger cache sizes and higher associativity reduced miss rates, particularly for instruction-heavy workloads like Dhrystone. While the plugin’s system-wide scope limits insights into enclave-specific cache activity, the trends suggest that cache tuning can meaningfully influence enclave performance.

Importantly, the relationship between memory allocation and PMP usage is nuanced. Larger enclave memory regions may require multiple PMP entries if the memory cannot be described contiguously in a single PMP register, effectively reducing the total number of enclaves supported simultaneously. This interaction between memory layout and PMP capacity represents a critical consideration for enclave deployment strategies.
