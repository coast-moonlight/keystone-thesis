\chapter{Benchmarking Evaluation}
\label{chap:benchmarking}

This chapter presents an in-depth performance evaluation of the selected workloads executed within the Keystone enclave framework. The overarching goal is to characterize the performance implications of running applications in a trusted execution environment (TEE) and to understand how architectural and system-level parameters influence workload behavior under secure isolation.

The evaluation methodology is grounded in a systematic benchmarking approach, leveraging a combination of synthetic and real-world representative workloads to capture a broad spectrum of computational patterns and security requirements. Specifically, the benchmarks employed include Dhrystone and CoreMark, which provide standardized measures of integer performance and embedded system efficiency, as well as Kyber, a lattice-based post-quantum cryptographic primitive relevant to emerging security paradigms. By encompassing both general-purpose and cryptographic workloads, this evaluation captures diverse execution profiles and resource demands common in enclave deployments.

Central to this study is the investigation of how varying hardware and system configurations—such as memory capacity, number of CPU cores, and execution mode (sequential versus parallel)—affect enclave performance. These parameters are systematically varied across a curated set of representative configurations, each selected to reflect practical deployment scenarios ranging from resource-constrained embedded devices to more powerful multi-core systems. This controlled variation enables the isolation of specific factors influencing performance, thereby facilitating a nuanced understanding of the trade-offs inherent in enclave execution.

Performance metrics are collected at multiple levels, including execution time, throughput, cache behavior, and system-level events such as context switches and page faults. Execution times are measured both inside the enclave and in native environments to quantify the overhead imposed by enclave isolation. Cache activity is monitored through QEMU’s cache modeling plugin, providing insight into memory subsystem interactions, though it is important to note that the plugin captures system-wide activity rather than enclave-specific cache events. Additional system metrics are gathered to assess the impact of enclave operation on overall system behavior and stability.

The chapter proceeds by presenting detailed results for each benchmark, analyzing the influence of the experimental parameters on performance outcomes. Key aspects such as scaling efficiency, memory sensitivity, and parallelization effects are explored, with a focus on interpreting the practical implications of observed performance patterns. Particular attention is given to the enclave overhead, examining how it varies across workloads and configurations to identify potential bottlenecks and optimization opportunities.

Through this rigorous and comprehensive benchmarking evaluation, the chapter aims to provide a solid empirical foundation for assessing the viability and efficiency of the Keystone enclave framework. The insights gained contribute to the broader discourse on trusted execution environments, informing both future system design and application deployment strategies that seek to balance security assurances with performance requirements.


\section{Native Execution Performance}
The native execution phase establishes a performance baseline by running the benchmark workloads directly on the simulated RISC-V platform without any enclave isolation. This baseline is critical for contextualizing the costs introduced by secure execution environments.

The Dhrystone and CoreMark benchmarks, widely used to assess general-purpose processor performance, exhibited clear scaling trends in accordance with the underlying hardware configurations. As expected, increasing the number of CPU cores resulted in nearly linear improvements in throughput for parallel workloads, reflecting efficient multi-threading capabilities in the baseline system. Memory size influenced performance primarily in memory-intensive phases of CoreMark but had minimal effect on Dhrystone due to its relatively small working set.

Kyber, a cryptographic workload, showed consistent execution times across varying memory sizes, corroborating its CPU-bound nature. Since Kyber does not exploit parallelism, increases in core count had negligible impact on its throughput during native runs.

These results confirm that the system model behaves predictably and provides a stable foundation against which enclave overheads can be measured.

\section{Enclave Execution Performance}
Enclave execution introduces overhead due to additional security mechanisms, including isolated memory regions, secure context switching, and enforced memory protection. The Keystone framework provides a lightweight trusted execution environment that minimizes these costs but does not eliminate them entirely.

Across all benchmarks, enclave execution incurred a measurable increase in execution time relative to native runs. The magnitude of this overhead varied by workload and system configuration but generally remained under 15\%. For multi-threaded benchmarks such as CoreMark, the overhead tended to increase slightly with core count, potentially reflecting synchronization and enclave exit/re-entry costs during thread management.

Context switch analysis revealed that enclaves generated more frequent transitions between secure and non-secure worlds, contributing to latency. Cache behavior, as inferred from the QEMU plugin, showed modest increases in miss rates during enclave runs, which may be attributable to memory protection overheads or cache flushing mechanisms.

Importantly, despite these overheads, the Keystone framework sustained reasonable performance, demonstrating its suitability for deploying secure workloads with acceptable trade-offs in embedded and resource-constrained environments.

\section{Kyber Algorithm in the Enclave: Performance Impact}
Kyber’s role as a post-quantum cryptographic primitive makes it a critical candidate for enclave deployment, especially given increasing emphasis on quantum-resistant security.

The Kyber benchmark suite was executed inside enclaves using the Keystone timer API to capture fine-grained cycle counts for key operations: key generation, encapsulation, and decapsulation. Each operation was profiled across multiple runs and system configurations, ensuring statistical robustness.

Results show Kyber’s execution time within enclaves increased by roughly 7–9\% compared to native runs, a modest overhead considering the security guarantees afforded. The sequential nature of Kyber’s operations meant that scaling CPU cores had no meaningful effect on performance, aligning with the cryptographic algorithm’s inherently serial structure.

Memory profiling indicated stable usage patterns without signs of excessive contention or paging. Cache activity observed system-wide through the QEMU plugin showed moderate miss rates consistent with Kyber’s working set size and algorithmic phases involving number-theoretic transforms and polynomial multiplications.

Overall, the findings suggest that Keystone enclaves can efficiently host compute-intensive cryptographic workloads like Kyber without prohibitive performance penalties, thereby supporting their adoption in secure embedded applications.

\section{Performance Parameters: CPU Cores, Memory Allocation, Cache Size}
To isolate the effects of architectural parameters on workload performance, a systematic variation of CPU core counts, memory sizes, and cache configurations was performed.

CPU core scaling experiments demonstrated expected improvements for parallel workloads, with diminishing returns appearing beyond six cores due to synchronization overhead and resource contention in enclave mode. Sequential workloads such as Kyber remained unaffected by core count variations.

Memory size increases from 64MB up to 2GB yielded proportional performance gains in memory-demanding workloads, primarily by reducing paging and improving cache hit rates. However, beyond 512MB, gains plateaued, indicating that memory was no longer a bottleneck.

Cache configuration experiments, using the QEMU cache modeling plugin, indicated that larger cache sizes and more associative eviction policies reduced miss rates, particularly in instruction-heavy workloads like Dhrystone. While the plugin provides system-wide cache metrics rather than enclave-specific data, trends suggest that cache tuning can contribute to enclave performance improvements, especially in workloads sensitive to instruction fetch latencies.

Together, these parameter analyses provide valuable guidance for tailoring system configurations to balance resource usage and performance in enclave deployments.
\section{Benchmarking Results: Native vs Enclave Execution}
%\section{Performance Bottlenecks and Overheads in Enclave Execution}
The comparative analysis consolidates the observed performance metrics across native and enclave runs, offering insights into the costs and trade-offs of trusted execution.

Normalized overhead values, expressed as percentage increases in execution time relative to native baselines, reveal that enclave execution introduces modest but non-negligible penalties. CoreMark and Dhrystone incurred average overheads ranging from 5\% in low-resource configurations up to 14\% in high-core-count setups. Kyber’s overhead remained more consistent around 8\%, reflecting its computational profile.

Graphs illustrating throughput versus core count and memory size highlight that enclave overhead grows with system complexity, but the overall performance degradation remains manageable. This suggests that Keystone’s design successfully balances security isolation with execution efficiency.

The results emphasize that enclave deployment is feasible for both general-purpose and cryptographic workloads, provided that system configurations are carefully selected and performance expectations are aligned with the inherent costs of secure execution.