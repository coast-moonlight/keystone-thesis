\chapter{Benchmarking Results and Observations}
\label{chap:benchmarking}

This chapter presents a detailed empirical evaluation of the Keystone enclave framework, with a specific focus on its performance characteristics under a range of representative workloads. The primary aim is to quantify the computational and system-level overheads introduced by secure enclave execution on RISC-V platforms, and to explore how these overheads are influenced by underlying hardware configurations and architectural constraints. By systematically comparing enclave-based execution against native (non-isolated) execution, we aim to identify the key trade-offs and limitations that developers and system architects must consider when adopting enclaves for trusted execution.

To achieve this, we deploy a carefully selected set of benchmark workloads that span both general-purpose and security-critical application domains. These include the Dhrystone and CoreMark benchmarks—commonly used to assess CPU performance and embedded system efficiency—as well as the Kyber post-quantum cryptographic algorithm, which provides a real-world workload with stringent security and computational requirements. Together, these benchmarks provide a diverse and comprehensive view of how Keystone enclaves behave across different execution contexts.

The benchmarking process is structured to examine four interrelated dimensions. First, we establish a native performance baseline to isolate the impact of enclave-related operations. Second, we evaluate the performance of the same workloads when executed within enclaves, highlighting the latency and resource costs incurred by isolation mechanisms such as Physical Memory Protection (PMP) and context switching. Third, we perform a workload sensitivity analysis, comparing the behavior of different benchmark types—sequential vs. parallel, lightweight vs. compute-intensive—to determine how workload characteristics affect enclave performance. Finally, we assess the influence of key hardware parameters, including CPU core count, memory size, and cache configuration, on both native and enclave execution.

Through these experiments, we uncover both expected and non-trivial behaviors. For instance, while enclave overheads are generally modest—typically under 15\%—they are highly sensitive to parallelism and memory access patterns. Furthermore, we identify critical bottlenecks, such as the fixed number of PMP entries available for isolating memory regions, which directly limit enclave concurrency and scalability.

The results presented in this chapter serve as the empirical foundation for the interpretive analysis in the next chapter. By rigorously characterizing the performance profile of Keystone enclaves, we establish a data-driven basis for understanding the broader implications of enclave deployment in secure system design, as well as informing practical strategies for optimization and resource management.

\section{Baseline: Native (Non-Enclave) Performance}

To establish a baseline for evaluating enclave overheads, we first executed all benchmarks natively on the simulated RISC-V platform without any enclave isolation. This phase allows us to measure the raw performance characteristics of the system and isolate the costs introduced solely by secure execution.

The Dhrystone and CoreMark benchmarks exhibited expected scaling behavior with respect to CPU cores and memory size. As CPU core count increased, throughput for these parallelizable benchmarks improved approximately linearly, reflecting efficient multi-threaded execution on the underlying hardware. Memory size impacted CoreMark more noticeably, particularly during memory-intensive phases, where increased memory reduced paging and improved cache hit rates. In contrast, Dhrystone’s smaller working set resulted in negligible sensitivity to memory size variations.

The Kyber cryptographic benchmark behaved differently. Due to its inherently sequential algorithmic structure, Kyber’s execution time remained relatively constant across changes in CPU core count. Likewise, memory size had minimal impact on Kyber performance, as its working set comfortably fit within allocated memory without inducing paging.

These native baseline results confirm the stability and predictability of the simulated system environment, providing a solid foundation for subsequent enclave overhead analysis.

\section{Enclave Execution Results and Overheads}

Execution within Keystone enclaves introduces overhead from multiple sources: memory isolation enforced by PMP, additional context switches to manage transitions between secure and non-secure execution states, and the general overhead of enforcing enclave boundaries at runtime.

Across all benchmarks, enclave execution resulted in increased execution times compared to native runs. Overhead magnitude varied by workload and system configuration but generally remained below 15\%. For parallel workloads such as CoreMark, overhead tended to increase with the number of CPU cores, likely due to increased synchronization demands and frequent enclave exit and re-entry operations during thread management. Dhrystone showed more modest overhead growth with core scaling.

Cache behavior, measured indirectly via QEMU’s cache modeling plugin, indicated modest increases in cache miss rates during enclave execution. Although this plugin provides system-wide cache metrics rather than enclave-specific data, the observed increases likely stem from PMP-related overheads such as cache flushing or restricted memory access patterns. These overheads are an expected consequence of maintaining strong memory isolation guarantees.

A critical observation during enclave execution was the enforcement and limitation of PMP resources. The fixed PMP register count on the QEMU virt platform fundamentally restricts the number of enclaves that can be concurrently instantiated. When the number of requested enclaves exceeds the 16 PMP entries, enclave creation fails. These failures are characterized by SBI call error codes and logged kernel errors indicating that no PMP registers are available. This limitation underscores the importance of considering PMP capacity in enclave orchestration and resource allocation.

\section{Workload Sensitivity Analysis (Kyber vs CoreMark vs Dhrystone)}

The Kyber post-quantum cryptographic algorithm is a representative workload for evaluating the performance impact of enclave execution on security-critical applications. Kyber consists of three primary operations—key generation, encapsulation, and decapsulation—each profiled extensively within enclaves using the Keystone timer API to capture precise cycle counts.

Execution within enclaves increased Kyber’s runtime by approximately 7–9\% compared to native execution. This overhead is modest given the cryptographic operations’ complexity and the security benefits provided by enclaves.

As with native execution, Kyber performance was insensitive to increases in CPU core count due to its inherently sequential design. Memory usage during enclave execution remained stable, with no indications of paging or contention. System-wide cache miss rates observed through QEMU remained consistent with Kyber’s memory access patterns, including phases dominated by number-theoretic transforms and polynomial arithmetic.

From a resource perspective, the requirement of one PMP entry per enclave constrains the number of parallel Kyber instances that can be simultaneously deployed. This constraint must be taken into account when designing systems requiring multiple isolated cryptographic operations.

\section{Impact of Hardware Parameters (Cores, Memory, Cache)}

To comprehensively assess how hardware configurations affect enclave performance, we systematically varied CPU core counts, memory allocations, and cache configurations.

Scaling the number of CPU cores yielded anticipated improvements for parallel workloads, with throughput gains plateauing beyond six cores due to synchronization overhead and increased enclave management complexity. Sequential workloads such as Kyber remained unaffected by core scaling.

Memory allocation experiments revealed that increasing memory size from 64MB to 2GB improved performance for memory-intensive benchmarks by reducing paging and increasing cache hit rates. However, beyond approximately 512MB, gains diminished, indicating sufficient memory provision for the workload’s working set.

Cache configuration impacts were evaluated using the QEMU cache modeling plugin. Larger cache sizes and higher associativity reduced miss rates, particularly for instruction-heavy workloads like Dhrystone. While the plugin’s system-wide scope limits insights into enclave-specific cache activity, the trends suggest that cache tuning can meaningfully influence enclave performance.

Importantly, the relationship between memory allocation and PMP usage is nuanced. Larger enclave memory regions may require multiple PMP entries if the memory cannot be described contiguously in a single PMP register, effectively reducing the total number of enclaves supported simultaneously. This interaction between memory layout and PMP capacity represents a critical consideration for enclave deployment strategies.
