\chapter{Methodology}
\label{chap:methodology}
The primary objective of this thesis is to assess the performance impact of Keystone’s enclave isolation mechanisms on typical embedded and systems-level workloads. Specifically, the study seeks to quantify the computational overhead introduced by executing applications within a Keystone enclave, as compared to their execution in a conventional, non-isolated environment. To achieve this, a series of carefully controlled experiments were designed and executed within a virtualized RISC-V system based on QEMU.

All benchmarks were executed in an emulated environment configured for the RV64GC architecture, which is representative of 64-bit RISC-V systems targeted by Keystone. The use of QEMU for emulation allowed for fine-grained control over the system environment, ensured repeatability, and eliminated the variability introduced by physical hardware, such as thermal throttling or hardware-specific optimizations. Although virtualized, the emulation accurately models instruction execution, privilege transitions, memory access patterns, and I/O behavior, making it suitable for preliminary performance evaluation.

To evaluate the impact of enclave execution, each benchmark was implemented as a standalone enclave application (referred to as an “eapp”), accompanied by a corresponding host application that manages enclave lifecycle events. The host application, running in user space, is responsible for initializing the enclave, loading the benchmark binary, invoking enclave entry points, and handling edge calls used for communication between the enclave and the host. The enclave application, in contrast, is isolated by the Keystone runtime and contains only the benchmark logic and internal data structures. This modular separation ensures that the same benchmark code can be executed both natively (without an enclave) and securely (within an enclave) without structural modification, enabling an accurate comparative analysis.

The evaluation involved executing each benchmark—Dhrystone and CoreMark—under two distinct configurations:

\begin{enumerate}
\item \textit{Native Execution:} The benchmark runs as a conventional user-space process within the QEMU-emulated Linux system, without invoking Keystone enclave services.
\item \textit{Enclave Execution:} The benchmark is loaded into a Keystone enclave, with isolation enforced by the Security Monitor using PMP.
\end{enumerate}

Each benchmark was run multiple times (typically ten iterations per configuration) to account for performance variability and ensure statistical robustness. For each run, metrics such as total execution time, throughput (measured in DMIPS for Dhrystone and iterations per second for CoreMark), and standard deviation were collected. The comparative analysis focused on the relative performance degradation observed in the enclave configuration, thereby providing a direct measurement of the cost of security.

This methodology enables a nuanced understanding of how Keystone’s isolation features influence real-world performance metrics. The controlled nature of the test environment, coupled with repeated measurement and the use of industry-standard benchmarks, ensures that the results are both meaningful and reproducible. By isolating the effect of enclave execution, this study contributes valuable insights into the trade-offs between security and efficiency in open-source TEE architectures.

\section{Experimental setup}
%The experimental setup utilizes the Keystone-enhanced QEMU emulator running on an Ubuntu host operating system. This configuration allows for flexible development and testing of Keystone enclaves in a controlled environment without requiring physical RISC-V hardware. The Ubuntu host provides a stable platform for building and deploying enclave binaries, and it facilitates the integration of the Keystone kernel driver and SDK.

The experimental setup employed in this study was carefully designed to facilitate reliable, repeatable testing of the Keystone Trusted Execution Environment (TEE) without the need for physical RISC-V hardware. To this end, a virtualized testbed was constructed using a RISC-V specific fork of the QEMU emulator, hosted on an Ubuntu 22.04 LTS system. This emulated environment provided a high degree of control over the system configuration, allowing for flexible testing of Keystone enclaves in isolation from physical hardware constraints, while still accurately modeling the behavior of a real RISC-V system.

QEMU, an open-source hardware emulator, was configured to emulate the RV64GC (64-bit general-purpose) RISC-V architecture, which is fully compatible with Keystone’s software stack. The choice of RV64 as the target architecture reflects Keystone’s intended deployment scenario on modern RISC-V platforms and allows full access to the enclave memory protection and privilege separation mechanisms supported by the Security Monitor.

The host operating system—Ubuntu 22.04—was chosen for its long-term support, stable kernel interfaces, and compatibility with the Keystone development toolchain. It served as the platform for building the QEMU binary, compiling the Keystone components, and orchestrating the virtual machine environment in which all benchmarks were executed.

The build process for Keystone relied on a combination of Make and Buildroot. Buildroot, an established tool for generating embedded Linux systems, was used to construct the root filesystem, cross-compile essential system libraries, and configure the Linux kernel used in the virtual guest. The Make-based build system provided streamlined control over component compilation and deployment, enabling reproducible builds of the Keystone runtime, Security Monitor, and associated kernel modules. During the build phase, key configuration parameters were selected, including the choice of QEMU as the target platform, the RV64 architecture, and the specific Buildroot configuration required to support the Keystone kernel driver and enclave runtime environment.

After successful compilation, the Keystone system was launched within QEMU using the command:

\begin{verbatim}
make run
\end{verbatim}

This command initiated the full virtual system, including the Keystone Security Monitor—responsible for managing enclave creation, destruction, and memory isolation—as well as a minimal Linux operating system running in supervisor mode within the QEMU guest. The Linux environment within QEMU was configured to support manual loading of the Keystone kernel driver, which was necessary to enable enclave functionality. Once the system was booted, the kernel driver was inserted using the following command:

\begin{verbatim}
modprobe keystone-driver
\end{verbatim}

This step completed the initialization of the enclave infrastructure, allowing benchmark binaries to be securely loaded into enclaves and executed with isolation enforced by the Security Monitor.

For debugging purposes, additional support was enabled through QEMU’s built-in GDB server. This allowed low-level inspection of the system state, including memory regions, control registers, and exception handlers. When debugging was required, Keystone was recompiled with debugging features enabled by setting the appropriate build flag, and execution was resumed using GDB connectivity:

\begin{verbatim}
KEYSTONE_DEBUG=y make run
make debug-connect
\end{verbatim}

Additional details on the build process, dependencies, and configuration options are available in the \textbf{Appendix}.

\section{Benchmarking tools and metrics}

To evaluate the performance impact of the Keystone Trusted Execution Environment (TEE) in a meaningful and comprehensive way, this study uses two well-established synthetic benchmarks: \textit{Dhrystone} and \textit{CoreMark}. Both are widely used in the embedded systems community and are well-supported in the RISC-V ecosystem, making them a natural fit for assessing how Keystone behaves in secure versus non-secure execution modes. These benchmarks were chosen because they capture a wide range of processor behaviors, particularly those relevant to secure enclave execution—such as integer arithmetic, control flow, and memory access patterns.

The Dhrystone benchmark has been around for decades and was originally designed to measure general-purpose processor performance, especially in environments where floating-point operations and file I/O are not a major concern. Its focus is on integer calculations and logical control structures—things like loops, function calls, and basic data handling—which are typical in many embedded and real-time applications. Because of its simplicity and minimal memory usage, Dhrystone provides a good baseline measure of a processor’s raw computing capability. For this study, the RISC-V version of Dhrystone was obtained from the official \texttt{riscv-tests} repository to ensure that it ran correctly on the target hardware. The benchmark was executed twice under identical conditions—once in a standard, non-secure mode, and once within a Keystone enclave. This setup made it possible to measure the specific performance overhead introduced by Keystone’s security features, such as memory isolation and the cost of switching between secure and non-secure execution contexts.

Dhrystone results are typically expressed in terms of "Dhrystones per second," which simply counts how many iterations of the benchmark are completed in one second. To make the numbers more meaningful across different architectures, these results are often normalized to Dhrystone MIPS (DMIPS), which estimates how many millions of instructions per second the processor is effectively delivering. This helps smooth out differences between instruction sets and gives a more comparable figure. In the context of this study, comparing DMIPS between enclave and non-enclave runs highlights how much raw performance is lost (or retained) when using the TEE.

To get a broader picture of system performance—particularly one that includes moderate memory use and more complex workloads—the CoreMark benchmark \cite{gal2012exploring} was also used. Developed by the Embedded Microprocessor Benchmark Consortium (EEMBC), CoreMark \cite{gal2012exploring} was designed to overcome some of the limitations of Dhrystone by providing a workload that’s a bit closer to real-world embedded applications. It includes tasks like linked list manipulation, matrix operations, and finite state machine processing, all of which are common in software running on embedded systems. While CoreMark \cite{gal2012exploring} is still CPU-focused, it introduces a moderate level of memory interaction through dynamic data structures, which allows it to reveal performance issues that Dhrystone might miss—especially those related to cache usage or memory protection features within a TEE.

The version of CoreMark \cite{gal2012exploring} used in this study came from the SiFive repository available on GitHub at \url{https://github.com/sifive/benchmark-coremark}, which provides a version optimized for RISC-V systems and ready to work with Keystone. Like Dhrystone \cite{weiss2002dhrystone}, CoreMark \cite{gal2012exploring} was run in both native and enclave modes to compare performance under each. Its results are reported in terms of "Iterations Per Second" (IPS), which tells us how many complete runs of the benchmark are executed every second. Since CoreMark includes both arithmetic and memory operations, IPS gives a more holistic view of how the system is performing—not just the processor, but also the memory subsystem.

Beyond the main performance numbers (DMIPS for Dhrystone and IPS for CoreMark), this study also looked at a few supporting metrics to better understand system behavior. The first is overall execution time, which refers to how long it takes (in real-world, wall-clock time) to run each benchmark. Comparing execution time in enclave vs. non-enclave modes helps quantify latency introduced by secure execution. This kind of delay might come from things like extra memory checks, context switching, or other internal mechanisms used to isolate the enclave from the rest of the system.

Another important metric is variability—specifically, the standard deviation of benchmark results across multiple runs. Each benchmark was executed several times under identical conditions, and the standard deviation was calculated to determine how consistent the performance was. Low standard deviation means the system performs reliably from run to run, while high deviation might indicate that performance is sensitive to system noise, scheduling delays, or other environmental factors.

Both Dhrystone and CoreMark are CPU-intensive, which is exactly what makes them useful in this context. Dhrystone focuses narrowly on integer operations and control flow, giving a clean look at raw processor performance. CoreMark, on the other hand, introduces a bit more complexity and memory interaction, which makes it better suited for revealing the broader impact of Keystone’s security mechanisms. Neither benchmark uses file I/O or persistent storage, which means that this study does not explore storage-related features of the TEE, such as secure disk access or file encryption. That’s outside the scope here and would require a different kind of benchmarking approach.

By using both benchmarks together—one simple and focused, the other broader and more realistic—this study creates a well-rounded picture of how Keystone affects performance. The addition of execution time and variability analysis provides further context, helping to separate consistent performance overhead from sporadic slowdowns or system noise. Overall, this methodology provides a clear, detailed view of the trade-offs involved when securing execution through a TEE on RISC-V platforms.

\section{Parameter Variation Strategy}
\label{sec:param-variation}

To examine the influence of microarchitectural cache parameters on enclave performance, this study employed a structured variation of Level-1 (L1) cache configurations using QEMU's cache modeling capabilities. The experimental objective was to isolate the impact of cache size, associativity, eviction policy, and core count on the performance of secure and non-secure workloads in a controlled emulation environment.

Cache modeling was enabled through the QEMU TCG plugin framework \cite{mandour2021cache}, which simulates idealized, per-core instruction and data caches. While not microarchitecturally cycle-accurate, this plugin supports a configurable cache hierarchy, enabling systematic exploration of cache behaviors relevant to trusted execution environments.

Four key experimental scenarios were defined:

\begin{enumerate}
    \item \textbf{Baseline Configuration:} A minimal cache setup with 8\,KB instruction and data caches, 4-way associativity, and LRU eviction.
    \item \textbf{Larger Cache Scenario:} An expanded L1 cache configuration with 32\,KB size and 8-way associativity to evaluate the effect of reduced cache miss rates.
    \item \textbf{Multicore Simulation:} A four-core configuration with private per-core caches to study inter-core effects such as contention and cache coherence approximations.
    \item \textbf{Eviction Policy Sensitivity:} A fixed-size cache using alternative eviction strategies (FIFO and random) to assess their influence on enclave memory behavior.
\end{enumerate}

All scenarios maintained a fixed block size of 64 bytes to preserve spatial locality characteristics. The same set of benchmarks—Dhrystone and CoreMark—was executed under each configuration, both in native and enclave modes, to quantify differences in execution time, throughput, and cache performance.

The plugin-generated metrics included instruction and data cache accesses, miss rates, and identification of cache-thrashing instructions. While detailed configuration parameters, plugin invocation commands, and sample output logs are essential for reproduction, they are provided in Appendix~\ref{appendix:cache-plugin} to avoid interrupting the main methodological flow.

By varying one parameter set at a time while controlling for others, this approach facilitated a clear attribution of observed performance differences to specific architectural characteristics. The resulting data informed the broader analysis of how cache-aware design considerations can impact enclave performance in systems based on the Keystone framework.

\section{Data Collection and Analysis Procedures}
\label{sec:data-collection}

To ensure that the performance evaluation of enclave execution is both statistically robust and reproducible, a systematic approach was taken to data collection and analysis. All benchmarks were executed within the same virtualized RISC-V environment described in Section~\ref{chap:methodology}, with controlled cache parameters as outlined in Section~\ref{sec:param-variation}.

Each benchmark (Dhrystone and CoreMark) was executed under two configurations: native execution and enclave execution. For every configuration, a minimum of ten independent iterations was conducted to account for runtime variability. Execution time was measured using wall-clock timing as reported by the host Linux environment inside QEMU. Benchmark-specific performance metrics—Dhrystone MIPS (DMIPS) and CoreMark iterations per second (IPS)—were recorded after each run.

The collected data for each experiment included:

\begin{itemize}
    \item Total execution time (seconds)
    \item Benchmark throughput (DMIPS or IPS)
    \item Standard deviation and coefficient of variation
    \item Cache miss rates (instruction/data) when applicable
\end{itemize}

To mitigate the effects of transient system behavior, the first run of each benchmark was discarded as a warm-up iteration. The remaining samples were then averaged, and both the mean and standard deviation were computed. These values were used to assess not only the central tendency of performance, but also the stability of enclave execution under different cache configurations.

For cache-sensitive experiments, data was further enriched using QEMU's cache modeling plugin \cite{mandour2021cache}. This provided detailed logging of instruction and data accesses, cache miss counts, and identification of high-miss instructions. From these logs, miss rates were extracted and correlated with execution behavior. Selected logs and plugin outputs are included in Appendix~\ref{appendix:cache-plugin}.

Statistical analysis was conducted using standard descriptive statistics. Where relevant, normalized overhead values were computed to express enclave slowdown as a percentage increase in execution time or a reduction in throughput compared to the native baseline. These comparative metrics provide a clearer view of the cost of enclave isolation across various architectural conditions.

The consistent use of controlled environments, repeated measurements, and structured analysis ensures that the results presented in subsequent chapters are both methodologically sound and analytically valid.
