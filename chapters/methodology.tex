\section{Experimental setup}
%The experimental setup utilizes the Keystone-enhanced QEMU emulator running on an Ubuntu host operating system. This configuration allows for flexible development and testing of Keystone enclaves in a controlled environment without requiring physical RISC-V hardware. The Ubuntu host provides a stable platform for building and deploying enclave binaries, and it facilitates the integration of the Keystone kernel driver and SDK.

The experimental setup employed in this study was carefully designed to facilitate reliable, repeatable testing of the Keystone Trusted Execution Environment (TEE) without the need for physical RISC-V hardware. To this end, a virtualized testbed was constructed using a RISC-V specific fork of the QEMU emulator, hosted on an Ubuntu 22.04 LTS system. This emulated environment provided a high degree of control over the system configuration, allowing for flexible testing of Keystone enclaves in isolation from physical hardware constraints, while still accurately modeling the behavior of a real RISC-V system.

QEMU, an open-source hardware emulator, was configured to emulate the RV64GC (64-bit general-purpose) RISC-V architecture, which is fully compatible with Keystone’s software stack. The choice of RV64 as the target architecture reflects Keystone’s intended deployment scenario on modern RISC-V platforms and allows full access to the enclave memory protection and privilege separation mechanisms supported by the Security Monitor.

The host operating system—Ubuntu 22.04—was chosen for its long-term support, stable kernel interfaces, and compatibility with the Keystone development toolchain. It served as the platform for building the QEMU binary, compiling the Keystone components, and orchestrating the virtual machine environment in which all benchmarks were executed.

The build process for Keystone relied on a combination of Make and Buildroot. Buildroot, an established tool for generating embedded Linux systems, was used to construct the root filesystem, cross-compile essential system libraries, and configure the Linux kernel used in the virtual guest. The Make-based build system provided streamlined control over component compilation and deployment, enabling reproducible builds of the Keystone runtime, Security Monitor, and associated kernel modules. During the build phase, key configuration parameters were selected, including the choice of QEMU as the target platform, the RV64 architecture, and the specific Buildroot configuration required to support the Keystone kernel driver and enclave runtime environment.

After successful compilation, the Keystone system was launched within QEMU using the command:

\begin{verbatim}
make run
\end{verbatim}

This command initiated the full virtual system, including the Keystone Security Monitor—responsible for managing enclave creation, destruction, and memory isolation—as well as a minimal Linux operating system running in supervisor mode within the QEMU guest. The Linux environment within QEMU was configured to support manual loading of the Keystone kernel driver, which was necessary to enable enclave functionality. Once the system was booted, the kernel driver was inserted using the following command:

\begin{verbatim}
modprobe keystone-driver
\end{verbatim}

This step completed the initialization of the enclave infrastructure, allowing benchmark binaries to be securely loaded into enclaves and executed with isolation enforced by the Security Monitor.

For debugging purposes, additional support was enabled through QEMU’s built-in GDB server. This allowed low-level inspection of the system state, including memory regions, control registers, and exception handlers. When debugging was required, Keystone was recompiled with debugging features enabled by setting the appropriate build flag, and execution was resumed using GDB connectivity:

\begin{verbatim}
KEYSTONE_DEBUG=y make run
make debug-connect
\end{verbatim}

Additional details on the build process, dependencies, and configuration options are available in the \textbf{Appendix}.

\section{Benchmarking tools and metrics}

To evaluate the performance impact of the Keystone Trusted Execution Environment (TEE) in a meaningful and comprehensive way, this study uses two well-established synthetic benchmarks: \textit{Dhrystone} and \textit{CoreMark}. Both are widely used in the embedded systems community and are well-supported in the RISC-V ecosystem, making them a natural fit for assessing how Keystone behaves in secure versus non-secure execution modes. These benchmarks were chosen because they capture a wide range of processor behaviors, particularly those relevant to secure enclave execution—such as integer arithmetic, control flow, and memory access patterns.

The Dhrystone benchmark has been around for decades and was originally designed to measure general-purpose processor performance, especially in environments where floating-point operations and file I/O are not a major concern. Its focus is on integer calculations and logical control structures—things like loops, function calls, and basic data handling—which are typical in many embedded and real-time applications. Because of its simplicity and minimal memory usage, Dhrystone provides a good baseline measure of a processor’s raw computing capability. For this study, the RISC-V version of Dhrystone was obtained from the official \texttt{riscv-tests} repository to ensure that it ran correctly on the target hardware. The benchmark was executed twice under identical conditions—once in a standard, non-secure mode, and once within a Keystone enclave. This setup made it possible to measure the specific performance overhead introduced by Keystone’s security features, such as memory isolation and the cost of switching between secure and non-secure execution contexts.

Dhrystone results are typically expressed in terms of "Dhrystones per second," which simply counts how many iterations of the benchmark are completed in one second. To make the numbers more meaningful across different architectures, these results are often normalized to Dhrystone MIPS (DMIPS), which estimates how many millions of instructions per second the processor is effectively delivering. This helps smooth out differences between instruction sets and gives a more comparable figure. In the context of this study, comparing DMIPS between enclave and non-enclave runs highlights how much raw performance is lost (or retained) when using the TEE.

To get a broader picture of system performance—particularly one that includes moderate memory use and more complex workloads—the CoreMark benchmark was also used. Developed by the Embedded Microprocessor Benchmark Consortium (EEMBC), CoreMark was designed to overcome some of the limitations of Dhrystone by providing a workload that’s a bit closer to real-world embedded applications. It includes tasks like linked list manipulation, matrix operations, and finite state machine processing, all of which are common in software running on embedded systems. While CoreMark is still CPU-focused, it introduces a moderate level of memory interaction through dynamic data structures, which allows it to reveal performance issues that Dhrystone might miss—especially those related to cache usage or memory protection features within a TEE.

The version of CoreMark used in this study came from the SiFive repository, which provides a version optimized for RISC-V systems and ready to work with Keystone. Like Dhrystone, CoreMark was run in both native and enclave modes to compare performance under each. Its results are reported in terms of "Iterations Per Second" (IPS), which tells us how many complete runs of the benchmark are executed every second. Since CoreMark includes both arithmetic and memory operations, IPS gives a more holistic view of how the system is performing—not just the processor, but also the memory subsystem.

Beyond the main performance numbers (DMIPS for Dhrystone and IPS for CoreMark), this study also looked at a few supporting metrics to better understand system behavior. The first is overall execution time, which refers to how long it takes (in real-world, wall-clock time) to run each benchmark. Comparing execution time in enclave vs. non-enclave modes helps quantify latency introduced by secure execution. This kind of delay might come from things like extra memory checks, context switching, or other internal mechanisms used to isolate the enclave from the rest of the system.

Another important metric is variability—specifically, the standard deviation of benchmark results across multiple runs. Each benchmark was executed several times under identical conditions, and the standard deviation was calculated to determine how consistent the performance was. Low standard deviation means the system performs reliably from run to run, while high deviation might indicate that performance is sensitive to system noise, scheduling delays, or other environmental factors.

Both Dhrystone and CoreMark are CPU-intensive, which is exactly what makes them useful in this context. Dhrystone focuses narrowly on integer operations and control flow, giving a clean look at raw processor performance. CoreMark, on the other hand, introduces a bit more complexity and memory interaction, which makes it better suited for revealing the broader impact of Keystone’s security mechanisms. Neither benchmark uses file I/O or persistent storage, which means that this study does not explore storage-related features of the TEE, such as secure disk access or file encryption. That’s outside the scope here and would require a different kind of benchmarking approach.

By using both benchmarks together—one simple and focused, the other broader and more realistic—this study creates a well-rounded picture of how Keystone affects performance. The addition of execution time and variability analysis provides further context, helping to separate consistent performance overhead from sporadic slowdowns or system noise. Overall, this methodology provides a clear, detailed view of the trade-offs involved when securing execution through a TEE on RISC-V platforms.

\section{Parameter Variation Strategy}

\begin{comment}
In order to conduct a comprehensive and methodologically sound performance evaluation of the Keystone Trusted Execution Environment (TEE), a structured parameter variation strategy was employed. The goal of this strategy was to explore how different system and workload configurations affect performance, both inside and outside of the enclave context. By systematically varying a set of key parameters, it becomes possible to isolate the sources of performance overhead and assess the sensitivity of the system to specific operational conditions.

A fundamental aspect of the evaluation involved comparing benchmark execution in two distinct contexts: native execution versus enclave execution. In the native configuration, benchmarks were executed directly within the emulated Linux environment running on QEMU, without invoking Keystone’s enclave infrastructure. This configuration provided a performance baseline that reflects the raw computational capability of the underlying system without the influence of TEE-specific isolation mechanisms. In contrast, enclave execution involved running the same benchmark binaries within Keystone-enforced secure containers. This allowed for a direct measurement of the overhead introduced by features such as memory isolation, limited system call interfaces, and the additional control flow transitions required by the Security Monitor.

To ensure statistical significance and reproducibility, each benchmark was executed multiple times under identical conditions. Specifically, each configuration (e.g., native or enclave) was run ten times, and the results were aggregated to compute average execution time, throughput (expressed as DMIPS for Dhrystone or IPS for CoreMark), as well as the standard deviation across runs. This repetition helped mitigate the impact of transient system noise or timing anomalies that can occur in virtualized environments, especially under QEMU emulation. Additionally, the inclusion of standard deviation in the reported results provided insights into the consistency and stability of the enclave performance.

Another important axis of variation was workload size. For the CoreMark benchmark, which allows for configurable execution parameters, experiments were conducted using different iteration counts and internal data structure sizes. This enabled the evaluation of how enclave performance scales with increasing computational intensity and memory utilization. Because enclave memory is subject to isolation and access restrictions, increasing the memory footprint can expose performance bottlenecks that would not be apparent under smaller workloads. While Dhrystone does not support direct workload scaling through runtime arguments, multiple versions of the benchmark were compiled with different loop unroll factors to simulate variation in workload complexity. Although less precise than CoreMark’s controls, this approach still provided a meaningful indication of how performance trends shift with increasing instruction throughput requirements.

The influence of system-level configuration options was also examined. Keystone offers various compile-time and runtime flags that affect behavior and performance, most notably the debug mode. When enabled, debug mode activates additional runtime checks, logging mechanisms, and diagnostic outputs intended to aid developers during enclave development and testing. These features, while useful during implementation, introduce measurable overhead. By executing benchmarks with and without debug mode enabled, the experiments were able to quantify the cost of enhanced observability and determine whether such features significantly degrade performance.

Memory allocation parameters were also varied to understand their role in shaping enclave performance. In particular, the heap and stack sizes assigned to benchmarks were adjusted to evaluate the sensitivity of the system to memory layout and availability. This was particularly relevant for CoreMark, which dynamically allocates and manipulates internal data structures. Larger memory allocations could potentially lead to increased cache pressure or page faults within the enclave, highlighting the importance of efficient memory management in TEE contexts.

Measurement accuracy was further reinforced by cross-validating timing results. Execution time was captured using both the benchmarks’ internal timing functions and QEMU’s emulated timer infrastructure. This dual-timing approach ensured that timing results were not distorted by emulator artifacts or low-resolution timers. In cases where discrepancies were observed, additional investigation was conducted to determine the root cause, and only consistent, corroborated results were included in the final analysis.

Finally, to explore the potential impact of system-level contention, a set of exploratory experiments introduced artificial background load during native execution. Although Keystone enclaves currently operate in a single-threaded, non-preemptive mode, testing under load offered preliminary insights into how enclave performance might be influenced in future multi-threaded or multi-tenant scenarios, especially in shared virtualized environments. While these tests were not a primary focus of the evaluation, they provided valuable context for interpreting variability in the results.
\end{comment}

\section{Data Collection and Analysis Procedures}