\chapter{Interpretation and Broader Implications}
\label{chap:discussion}

This chapter presents a critical analysis of the experimental findings discussed in the preceding chapters, with the goal of extracting deeper insights into the behavior, limitations, and practical consequences of enclave-based execution using the Keystone framework on RISC-V platforms. While the benchmarking results provide quantitative evidence of performance trends and system behavior, this chapter aims to interpret those findings in a broader architectural and systems-level context.

Specifically, we examine how various system parameters—such as CPU core count, memory configuration, cache hierarchy, and enclave workload characteristics—influence performance under isolation. We also discuss the architectural constraints imposed by RISC-V's Physical Memory Protection (PMP) mechanism, highlighting its central role in enforcing security while simultaneously acting as a scalability bottleneck. By analyzing the trade-offs between security guarantees and system throughput, we offer a nuanced understanding of the design space that system architects and security engineers must navigate when deploying enclaves in resource-constrained or high-concurrency environments.

In addition to synthesizing the key performance trends, this chapter explores the broader implications of enclave deployment for secure system design, particularly in scenarios involving cryptographic workloads, multi-tenant execution, and edge computing platforms. We assess the extent to which Keystone and PMP-based isolation mechanisms can be reliably scaled, and we consider practical strategies for overcoming current limitations through architectural or software-level optimizations.

Finally, the chapter concludes by formulating design recommendations and identifying future directions for research and development. These include enhancements to hardware features such as PMP flexibility, improved enclave management strategies at the software layer, and potential extensions to the Keystone framework to support more scalable and efficient trusted execution environments.

This chapter transitions from measurement to meaning: it connects the observed data to the architectural principles underpinning enclave execution, thereby informing both future research and practical deployment of secure computing systems based on open RISC-V architectures.

\section{Synthesis of Key Performance Trends}

A key observation from the evaluation is that enclave execution introduces moderate but measurable overheads relative to native, non-isolated execution. These overheads primarily stem from two factors: the enforcement of memory isolation through Physical Memory Protection (PMP), and the additional control flow transitions between enclave and host contexts. Across a range of benchmark scenarios—including general-purpose integer computations and cryptographic workloads—the performance penalty typically remains below 15\%. However, this aggregate figure conceals significant variance, which is highly dependent on workload characteristics, system architecture, and execution configuration. Variables such as the number of processor cores, memory availability, and cache configuration exert a substantial influence on the observed overhead.

A second and arguably more fundamental insight concerns the limitations imposed by hardware-enforced resource constraints, particularly the finite number of PMP entries. In the QEMU \texttt{virt} platform used for this study, the system supports a maximum of 16 PMP entries. Each enclave requires at least one such entry to define a protected memory region. This architectural constraint creates a hard upper bound on the number of enclaves that can be concurrently instantiated. When this limit is exceeded, enclave creation fails deterministically, with the system reporting specific Supervisor Binary Interface (SBI) error codes and kernel log messages. This result highlights a critical bottleneck for enclave scalability that arises not from software inefficiencies, but from underlying hardware design choices. In practical deployments—particularly in multi-tenant or concurrent environments such as edge nodes or confidential cloud computing platforms—PMP exhaustion could severely limit system utility unless proactively mitigated.

The analysis further reveals that enclave performance is highly sensitive to workload type. Sequential, compute-bound tasks such as the Kyber post-quantum cryptographic algorithm exhibit relatively low performance degradation when executed within an enclave. This is attributable to Kyber’s predictable memory access patterns and minimal need for inter-thread synchronization. By contrast, parallelizable workloads like CoreMark demonstrate increasing overhead as core count rises, driven by factors such as synchronization costs, increased enclave context switching, and potential contention in PMP entry allocation. These results suggest that enclave performance optimization must be workload-aware, with particular consideration given to application parallelism and memory behavior.

Finally, the relationship between system resource allocation—specifically CPU cores, memory capacity, and cache hierarchy—and enclave performance is nuanced. Increasing the number of processor cores beyond a certain threshold yields diminishing performance returns due to synchronization overhead and enclave management complexity. While increasing memory allocation generally improves performance by reducing page faults and improving cache locality, it may also exacerbate PMP usage if memory regions are fragmented. Although QEMU provides only a high-level approximation of cache behavior, the observed trends suggest that cache size and associativity can significantly influence enclave performance for instruction- and memory-intensive workloads. These interdependencies underscore the need for a holistic, system-level tuning approach when deploying TEEs in real-world scenarios.

The benchmarking results confirm that Keystone enclaves, while effective at enforcing isolation, require careful system configuration and architectural awareness to ensure scalable and efficient performance. The findings serve as a foundation for both immediate system tuning and future architectural innovation aimed at overcoming the current limitations of enclave-based trusted execution on RISC-V.

\section{Hardware Bottlenecks and PMP Utilization Strategies}
A central and recurring theme throughout this study is the pivotal role played by the RISC-V Physical Memory Protection (PMP) mechanism in determining the scalability, flexibility, and reliability of secure enclave execution in the Keystone framework. Although PMP is indispensable for maintaining strong memory isolation—arguably the most critical security property in any Trusted Execution Environment (TEE)—it simultaneously introduces significant architectural constraints that place hard limits on system scalability. These limitations become especially apparent in environments that demand the concurrent instantiation of multiple enclaves, such as secure edge computing deployments, cloud-based confidential workloads, and multi-tenant embedded systems.

PMP operates by establishing fine-grained access control policies over physical memory ranges. These policies are enforced in hardware and serve to prevent unauthorized access by less privileged software layers, such as applications running in Supervisor or User mode. In the Keystone architecture, enclaves rely on PMP to ensure that their memory regions remain inaccessible to both the host operating system and other enclaves. Each protected memory region is described by a PMP entry, and these entries are programmed through control and status registers (CSRs) accessible only from the highest privilege level—Machine mode (M-mode). Each PMP entry specifies a memory region, along with the access permissions allowed (read, write, execute), and once configured, becomes an unforgeable part of the system’s isolation model.

However, the number of available PMP entries is not dynamic—it is a fixed architectural parameter determined by the processor implementation. In our experimental setup based on the widely used QEMU \texttt{virt} platform for RISC-V, this number is statically defined as 16. This finite resource pool becomes a fundamental limiting factor in the Keystone system because \textbf{each enclave requires at least one PMP entry} to establish its isolated memory region. In more complex memory layouts—especially those that are large or fragmented—multiple PMP entries may be needed to define non-contiguous or irregularly sized memory regions, further exacerbating resource pressure.

This constraint manifested clearly in our benchmarking experiments. When the number of concurrently requested enclaves exceeded the available PMP entries, enclave creation failed immediately. These failures were not silent; they were explicitly reported by the system in the form of Supervisor Binary Interface (SBI) error codes and verbose kernel log messages, providing clear indicators of PMP exhaustion. Typical output during such events included:

\begin{lstlisting}[language=bash,caption={Error messages during enclave creation and destruction}]
napot_region_init: No available PMP register
keystone_create_enclave: SBI call failed with error code 100003
fatal: cannot destroy enclave: SBI failed with error code 100005
pmp_unset: Invalid PMP region index
pmp_region_free_atomic: Invalid PMP region index
\end{lstlisting}

These logs reflect a hard architectural ceiling that is entirely independent of other system resources such as CPU cores or DRAM availability. Even in scenarios where there is ample memory and processing capacity to support additional enclaves, the inability to allocate a free PMP entry effectively blocks the deployment of further isolated workloads. This introduces a non-obvious, hardware-enforced bottleneck that could significantly impact the reliability and scalability of enclave-enabled systems.

Moreover, the situation is further complicated by the way PMP regions are encoded. PMP supports a limited set of region encoding formats, with the most commonly used being the Naturally Aligned Power-of-Two (NAPOT) mode. This format requires that protected memory regions be both naturally aligned and sized as a power of two. While this encoding simplifies hardware enforcement, it imposes rigid constraints on memory layout. For example, if an enclave's memory region spans an irregular range—such as 192 KB starting at a non-aligned address—it cannot be expressed with a single PMP entry. In such cases, the region must be broken down into smaller, aligned sub-regions, each requiring its own PMP entry. This fragmentation effect leads to inefficient utilization of available PMP entries and further reduces the maximum number of enclaves that can coexist.

To address these challenges, both \textbf{hardware-level} and \textbf{software-level} mitigation strategies must be considered:

\begin{itemize}
    \item \textbf{Hardware approaches} include increasing the number of available PMP entries. While this would require architectural changes and potentially more silicon real estate, it could dramatically improve enclave concurrency. Alternatively, future iterations of the RISC-V PMP specification could explore more flexible memory region encodings, allowing for more expressive and compact representations of protected regions, thereby reducing fragmentation.
    \item \textbf{Software techniques} include dynamic PMP entry management, where PMP entries are reused or reconfigured at runtime based on the current workload and enclave activity. This requires careful coordination to avoid security violations but can enable more flexible use of the limited register set. Another technique is enclave multiplexing, where multiple lightweight tasks or applications are combined into a single enclave, reducing the total number of enclaves required. Additionally, memory layout optimization—aligning and sizing enclave memory regions to match NAPOT constraints—can significantly reduce the number of PMP entries consumed per enclave.
\end{itemize}

From a systems engineering perspective, these limitations have real-world consequences. In cloud environments, where tenants may each be assigned their own isolated enclave for security, PMP constraints could become a source of contention, limiting the number of clients that can be served simultaneously. In embedded or edge computing devices with strict memory constraints, careful tuning of enclave footprints and memory allocation strategies will be necessary to ensure system robustness.

While the PMP mechanism is undeniably a cornerstone of the Keystone security model, it also introduces a set of hard limitations that impact the practical deployment and scaling of enclaves. Understanding and addressing these limitations—both through hardware evolution and smarter software resource management—is essential for building scalable, secure computing platforms based on the RISC-V architecture.

\section{Implications for Secure System Design and Cryptographic Workloads}

\section{Recommendations and Considerations for Future TEE Deployments}