\chapter{Interpretation and Broader Implications}
\label{chap:discussion}

This chapter presents a critical analysis of the experimental findings discussed in the preceding chapters, with the goal of extracting deeper insights into the behavior, limitations, and practical consequences of enclave-based execution using the Keystone framework on RISC-V platforms. While the benchmarking results provide quantitative evidence of performance trends and system behavior, this chapter aims to interpret those findings in a broader architectural and systems-level context.

Specifically, we examine how various system parameters—such as CPU core count, memory configuration, cache hierarchy, and enclave workload characteristics—influence performance under isolation. We also discuss the architectural constraints imposed by RISC-V's Physical Memory Protection (PMP) mechanism, highlighting its central role in enforcing security while simultaneously acting as a scalability bottleneck. By analyzing the trade-offs between security guarantees and system throughput, we offer a nuanced understanding of the design space that system architects and security engineers must navigate when deploying enclaves in resource-constrained or high-concurrency environments.

In addition to synthesizing the key performance trends, this chapter explores the broader implications of enclave deployment for secure system design, particularly in scenarios involving cryptographic workloads, multi-tenant execution, and edge computing platforms. We assess the extent to which Keystone and PMP-based isolation mechanisms can be reliably scaled, and we consider practical strategies for overcoming current limitations through architectural or software-level optimizations.

Finally, the chapter concludes by formulating design recommendations and identifying future directions for research and development. These include enhancements to hardware features such as PMP flexibility, improved enclave management strategies at the software layer, and potential extensions to the Keystone framework to support more scalable and efficient trusted execution environments.

This chapter transitions from measurement to meaning: it connects the observed data to the architectural principles underpinning enclave execution, therebyinforming both future research and practical deployment of secure computing systems based on open RISC-V architectures.

\section{Synthesis of Key Performance Trends and Hardware Bottlenecks}
\label{sec:synthesis}
The performance benchmarking conducted in the previous chapter revealed important trends regarding the execution of secure enclaves on RISC-V platforms using the Keystone framework. These trends offer both confirmation of expected behavior and highlight areas where performance is highly sensitive to various system parameters, particularly when isolating workloads for security purposes.

A key observation from our experiments is the consistent but moderate performance overhead introduced by enclave execution. This overhead, typically in the range of 10\% to 15\%, is a direct consequence of the memory isolation provided by the Physical Memory Protection (PMP) mechanism and the transition between host and enclave execution contexts. These overheads are not uniform, however; they depend heavily on workload characteristics, memory access patterns, and system resource configuration.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=14pt,
    width=10cm,
    height=6cm,
    ymin=0, ymax=120,
    ylabel={Relative performance (\%)},
    symbolic x coords={CoreMark,Dhrystone,Kyber},
    xtick=data,
    ymajorgrids,
    grid style=dashed,
    nodes near coords,
    every node near coord/.append style={font=\footnotesize, /pgf/number format/fixed},
    pattern color=black
]
\addplot[pattern=dots] coordinates {(CoreMark,95) (Dhrystone,62) (Kyber,37)};
\addplot[pattern=horizontal lines] coordinates {(CoreMark,100) (Dhrystone,100) (Kyber,100)};
\legend{Enclave,Native}
\end{axis}
\end{tikzpicture}
\caption{Relative performance of enclave vs.\ native execution. Overhead is modest for compute-bound workloads, severe for memory-bound Kyber.}
\end{figure}

For compute-bound tasks, such as those evaluated using CoreMark and Dhrystone, the overhead remains relatively modest. These workloads, which primarily perform integer arithmetic with relatively predictable memory access patterns, suffer minimal degradation. The enclave environment's cost is absorbed through control flow transitions, but since these benchmarks are not particularly memory-intensive, the additional latency incurred by memory isolation and context switches does not manifest as a major bottleneck.

However, workloads with more intensive memory access or intricate memory patterns, such as Kyber (a post-quantum cryptographic algorithm), experience much higher overhead. The Kyber algorithm, which involves significant pointer dereferencing and array indexing, incurs a significant penalty as every memory access is subject to additional security checks and encryption layers. This is compounded by the latency-sensitive nature of cryptographic workloads, where even small delays can result in significant performance degradation. The nearly threefold slowdown in enclave execution compared to native execution is primarily due to the added latency from memory encryption and integrity checks, which are inherently more expensive for memory-intensive algorithms.

This analysis underscores that enclave performance is not solely dependent on raw processing power but is intricately linked to the memory behavior of workloads. Sequential, compute-bound tasks like Dhrystone and CoreMark can endure the overhead of enclave execution without significant slowdowns, whereas memory-bound workloads such as Kyber suffer disproportionately due to frequent memory accesses that require validation, encryption, and integrity checks.

An interesting trend emerges when examining system resource allocation impacts on enclave performance. Increasing the number of CPU cores generally improves performance for multi-threaded applications; however, gains plateau beyond a certain threshold due to overhead introduced by context switching between enclave and host execution contexts and the need for thread synchronization within enclaves. This phenomenon is particularly evident in parallelizable workloads, such as CoreMark.

% \begin{figure}[htbp]
% \centering
% % Use minipages to place plots side by side
% \begin{minipage}{0.48\textwidth}
% \centering
% \begin{tikzpicture}
% \begin{axis}[
%     width=\linewidth,
%     height=6cm,
%     xlabel={CPU cores},
%     ylabel={Relative performance (\%)},
%     xmin=1, xmax=8,
%     ymin=90, ymax=130,
%     xtick={1,...,8},
%     ymajorgrids,
%     grid style=dashed,
%     mark options={solid},
%     nodes near coords,
%     every node near coord/.append style={font=\footnotesize, /pgf/number format/fixed}
% ]
% \addplot[pattern=dots,pattern color=black,mark=*] coordinates {
% (1,100) (2,108) (3,115) (4,120) (5,122) (6,123) (7,123) (8,123)
% };
% \end{axis}
% \end{tikzpicture}
% \caption*{(a) Scaling of a parallelisable workload (CoreMark) with CPU cores. Gains plateau beyond 4 cores due to enclave context-switch and synchronization overhead.}
% \end{minipage}
% \hfill
% \begin{minipage}{0.48\textwidth}
% \centering
% \begin{tikzpicture}
% \begin{axis}[
%     width=\linewidth,
%     height=6cm,
%     xlabel={Memory capacity (MB)},
%     ylabel={Relative performance (\%)},
%     xmin=64, xmax=512,
%     ymin=90, ymax=120,
%     ymajorgrids,
%     grid style=dashed,
%     mark options={solid},
%     nodes near coords,
%     every node near coord/.append style={font=\footnotesize, /pgf/number format/fixed}
% ]
% \addplot[pattern=north east lines,pattern color=black,mark=*] coordinates {
% (64,100) (128,106) (256,112) (512,115)
% };
% \addplot[pattern=crosshatch,pattern color=black,mark=square*] coordinates {
% (64,95) (128,100) (256,105) (512,107)
% };
% \legend{Contiguous,Fragmented}
% \end{axis}
% \end{tikzpicture}
% \caption*{(b) Effect of increasing memory on enclave performance. Fragmentation reduces gains by consuming extra PMP entries.}
% \end{minipage}
% \caption{(a) CPU core scaling and (b) memory capacity impact on enclave performance.}
% \label{fig:performance-scaling}
% \end{figure}

\begin{figure}[htbp]
\centering
% --- Panel (a) ---
\begin{minipage}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\linewidth,
    height=6cm,
    xlabel={CPU cores},
    ylabel={Relative performance (\%)},
    xmin=1, xmax=8,
    ymin=90, ymax=130,
    xtick={1,...,8},
    ymajorgrids,
    grid style=dashed,
    mark options={solid},
    font=\small,
    legend style={font=\small, at={(0.5,-0.25)}, anchor=north, legend columns=2},
    nodes near coords,
    every node near coord/.append style={font=\scriptsize, /pgf/number format/fixed}
]
\addplot[pattern=dots,pattern color=black,mark=*] coordinates {
(1,100) (2,108) (3,115) (4,120) (5,122) (6,123) (7,123) (8,123)
};
\addplot[pattern=horizontal lines,pattern color=black,mark=square*] coordinates {
(1,100) (2,102) (3,103) (4,104) (5,104) (6,104) (7,104) (8,104)
};
\legend{Native,Enclave}
\end{axis}
\end{tikzpicture}
\subcaption{CoreMark scaling. Enclave performance plateaus early due to overhead.}
\end{minipage}\hfill
% --- Panel (b) ---
\begin{minipage}{0.48\textwidth}
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\linewidth,
    height=6cm,
    xlabel={Memory capacity (MB)},
    ylabel={Relative performance (\%)},
    xmin=64, xmax=512,
    ymin=90, ymax=120,
    xtick={64,128,256,512},
    ymajorgrids,
    grid style=dashed,
    mark options={solid},
    font=\small,
    legend style={font=\small, at={(0.5,-0.25)}, anchor=north, legend columns=1},
    nodes near coords,
    every node near coord/.append style={font=\scriptsize, /pgf/number format/fixed}
]
\addplot[pattern=dots,pattern color=black,mark=*] coordinates {
(64,100) (128,106) (256,112) (512,115)
};
\addplot[pattern=horizontal lines,pattern color=black,mark=square*] coordinates {
(64,95) (128,98) (256,100) (512,102)
};
\addplot[pattern=crosshatch,pattern color=black,mark=triangle*] coordinates {
(64,92) (128,94) (256,95) (512,96)
};
\legend{Native contiguous, Enclave contiguous, Enclave fragmented}
\end{axis}
\end{tikzpicture}
\subcaption{Memory scaling. Fragmentation limits gains by increasing PMP usage.}
\end{minipage}

\caption{Scaling effects on performance: (a) CPU core scaling for CoreMark; (b) memory capacity impact in native vs.\ enclave modes with contiguous and fragmented allocations.}
\label{fig:performance-scaling}
\end{figure}


Memory configuration also plays a critical role in determining enclave performance. Enclaves are sensitive to memory availability; increasing memory capacity typically improves performance by reducing page faults and improving cache locality. However, fragmented memory regions may require multiple PMP entries to define non-contiguous areas, exacerbating system overhead. The trade-off between memory allocation and PMP usage must therefore be carefully managed. Exhaustion of PMP entries leads to severe performance degradation, as additional enclaves cannot be instantiated regardless of available CPU or memory resources.

A crucial finding from our analysis is the scalability bottleneck introduced by the limited number of available PMP entries. In our setup, the maximum number of PMP entries is fixed at 16. Since each enclave requires at least one PMP entry to establish its isolated memory region—and more in fragmented cases—this limits the number of concurrent enclaves that can be instantiated. This is a significant concern in multi-tenant or high-concurrency environments where multiple enclaves may be required simultaneously.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=10cm,
    height=6cm,
    xlabel={PMP entries available},
    ylabel={Max concurrent enclaves},
    xmin=0, xmax=16,
    ymin=0, ymax=16,
    xtick={0,4,8,12,16},
    ymajorgrids,
    grid style=dashed,
    mark options={solid},
    nodes near coords,
    every node near coord/.append style={font=\footnotesize, /pgf/number format/fixed},
    clip=false
]

\addplot[
    pattern=crosshatch dots,
    pattern color=black,
    mark=*,
    thick
] coordinates {
    (0,0) (2,2) (4,4) (6,6) (8,8) (10,8) (12,8) (14,8) (16,8)
};

\fill[red!20,opacity=0.3] (8,0) rectangle (16,16);

\node[red, align=center, font=\footnotesize] at (axis cs:12,12) {PMP exhaustion \\ prevents \\ >8 enclaves};

% Vertical dashed line at x=4 with horizontal label
\draw[dashed, thick] (axis cs:4,0) -- (axis cs:4,16);
\node[anchor=south, font=\footnotesize] at (axis cs:4,16) {stability boundary};

\end{axis}
\end{tikzpicture}
\caption{Max concurrent enclaves increase with PMP entries up to 4 (stability boundary); beyond this, success rates degrade and cap at 8 due to PMP exhaustion.}
\end{figure}

This limitation manifests in our experiments where enclave creation fails when PMP entries are exhausted, even if other system resources remain plentiful. These failures are explicit, indicated by Supervisor Binary Interface (SBI) error codes and kernel log messages such as:

\begin{lstlisting}[language=bash,caption={Error messages during enclave creation and destruction}]
napot_region_init: No available PMP register
keystone_create_enclave: SBI call failed with error code 100003
fatal: cannot destroy enclave: SBI failed with error code 100005
pmp_unset: Invalid PMP region index
pmp_region_free_atomic: Invalid PMP region index
\end{lstlisting}

While the PMP mechanism is critical for enforcing isolation in secure enclave systems, its limited number of entries creates significant scalability challenges. Addressing these through combined hardware evolution and smarter software resource management is essential for building scalable, secure computing platforms on RISC-V.

\section{Implications for Secure System Design and Cryptographic Workloads}
\label{sec:implications}

The challenges highlighted in the previous section, particularly those related to the RISC-V Physical Memory Protection (PMP) mechanism, have significant implications for the design of secure systems, especially those that need to handle cryptographic workloads. As the demand for secure enclaves continues to grow—particularly in cloud environments, edge computing, and multi-tenant systems—the trade-offs between security and performance must be carefully balanced. While the ability to enforce strong memory isolation using PMP is critical for ensuring the confidentiality and integrity of sensitive data, it also introduces system bottlenecks that can severely impact performance, particularly for memory-bound tasks such as cryptography.

Cryptographic workloads, by their nature, are highly sensitive to both memory access patterns and latency. This sensitivity is exacerbated in secure enclave environments, where each memory access incurs overhead due to the enforcement of isolation policies and security checks. The previous section's analysis demonstrated that memory-bound workloads, such as post-quantum cryptography algorithms like Kyber, suffer disproportionately from the overhead introduced by PMP. The large memory footprints and frequent memory accesses involved in these cryptographic operations significantly increase the performance degradation compared to compute-bound tasks like Dhrystone or CoreMark.

This issue becomes even more pronounced in multi-tenant environments where multiple enclaves are run concurrently. As the number of required PMP entries increases, system resources become strained, leading to potential failures in enclave instantiation when PMP resources are exhausted. Given that cryptographic operations are often required in scenarios that demand high throughput and low latency—such as in secure communication channels or confidential computing in cloud environments—this resource limitation can hinder the practical deployment of secure enclaves at scale.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1, transform shape]
\begin{axis}[
    ybar,
    bar width=10pt,
    width=0.9\linewidth, height=6cm,
    ylabel={PMP entries required},
    symbolic x coords={128\,KB aligned,192\,KB misaligned,300\,KB misaligned,384\,KB misaligned},
    xtick=data,
    x tick label style={rotate=20, anchor=east},
    ymajorgrids, grid style=dashed,
    enlarge x limits=0.15,
    legend style={at={(0.5,-0.3)},anchor=north,legend columns=2},
    pattern color=black,
    nodes near coords, every node near coord/.append style={font=\scriptsize, rotate=90, anchor=west}
]
\addplot[pattern=north east lines] coordinates {
(128\,KB aligned,1)
(192\,KB misaligned,2)
(300\,KB misaligned,2)
(384\,KB misaligned,1)
};
\addplot[pattern=crosshatch] coordinates {
(128\,KB aligned,2)
(192\,KB misaligned,3)
(300\,KB misaligned,3)
(384\,KB misaligned,2)
};
\legend{Contiguous mapping,Fragmented mapping}
\end{axis}
\end{tikzpicture}
\caption{Fragmentation and misalignment increase PMP entry usage, showing how non-ideal alignment leads to more PMP entries per enclave.}
\label{fig:pmp-fragmentation-entries}
\end{figure}

To address overhead and scalability challenges, \emph{enclave multiplexing} offers a promising software-level strategy by consolidating multiple lightweight workloads into a single enclave. This reduces the number of PMP entries required by reusing them across tasks, optimizing resource usage and improving scalability. An example of a multiplexed enclave task dispatcher is shown in Listing~\ref{lst:enclave-mux}.

\begin{lstlisting}[language=Python, caption={Multiplexed enclave task dispatcher.}, label={lst:enclave-mux}]
def enclave_dispatcher(task_id, input_data):
    if task_id == 1:
        return process_crypto(input_data)
    elif task_id == 2:
        return handle_key_store(input_data)
    elif task_id == 3:
        return run_diagnostics(input_data)
    else:
        raise ValueError("Unknown task")
\end{lstlisting}

Furthermore, the trade-off between memory usage and security introduces additional complexity in system design. Enclaves need to be carefully sized and aligned to fit within the constraints of PMP's encoding scheme, particularly the NAPOT (Naturally Aligned Power-Of-Two) encoding used to specify memory regions. For cryptographic workloads that often require non-contiguous memory layouts, inefficient PMP entry management leads to fragmentation and resource wastage, increasing overhead and limiting concurrent enclaves.

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[scale=1, transform shape,font=\small]
% Memory bar
\draw[thick] (0,0) rectangle (12,1);
\node[anchor=south] at (0,1.1) {Base};
\node[anchor=south] at (12,1.1) {Top};

% Needed regions (non-contiguous)
\draw[fill=gray!30] (0.5,0.1) rectangle (2.2,0.9) node[pos=.5] {\footnotesize R1};
\draw[fill=gray!30] (3.8,0.1) rectangle (4.6,0.9) node[pos=.5] {\footnotesize R2};
\draw[fill=gray!30] (7.0,0.1) rectangle (9.1,0.9) node[pos=.5] {\footnotesize R3};

% NAPOT allocations (outer boxes) and padding (hatched)
\draw[thick] (0.25,-0.3) rectangle (2.5,-1.1) node[pos=.5] {\footnotesize NAPOT fit R1};
\draw[pattern=north east lines,pattern color=black] (0.25,-0.3) rectangle (0.5,-1.1);
\draw[pattern=north east lines,pattern color=black] (2.2,-0.3) rectangle (2.5,-1.1);

\draw[thick] (3.5,-0.3) rectangle (5.1,-1.1) node[pos=.5] {\footnotesize NAPOT fit R2};
\draw[pattern=north east lines,pattern color=black] (3.5,-0.3) rectangle (3.8,-1.1);
\draw[pattern=north east lines,pattern color=black] (4.6,-0.3) rectangle (5.1,-1.1);

\draw[thick] (6.7,-0.3) rectangle (9.3,-1.1) node[pos=.5] {\footnotesize NAPOT fit R3};
\draw[pattern=north east lines,pattern color=black] (6.7,-0.3) rectangle (7.0,-1.1);
\draw[pattern=north east lines,pattern color=black] (9.1,-0.3) rectangle (9.3,-1.1);

\node[anchor=west] at (0,-1.5) {\footnotesize Hatched = padding/waste due to power-of-two alignment};
\end{tikzpicture}
\caption{Fragmentation overhead caused by mapping non-contiguous memory regions to NAPOT-aligned blocks. The hatched areas represent wasted padding required for alignment.}
\label{fig:napot-fragmentation}
\end{figure}

The memory layout complexity is also reflected in code structures like the custom memory layout shown in Listing~\ref{lst:memory-layout}, which attempts to align heap, stack, and code regions to NAPOT boundaries.

\begin{lstlisting}[language=C, caption={Custom memory layout aligned to NAPOT for enclave use.}, label={lst:memory-layout}]
struct enclave_memory_layout {
    void* heap_start;     // Aligned to 0x1000
    void* heap_end;
    void* stack_start;    // Aligned to 0x1000
    void* stack_end;
    void* code_region;    // Executable, aligned
};
\end{lstlisting}

At the hardware configuration level, PMP entries must be programmed carefully to respect alignment and permission constraints. Listing~\ref{lst:pmp-napot} illustrates a PMP configuration for a 4KB NAPOT-aligned enclave memory region, enabling read, write, and execute permissions.

\begin{lstlisting}[language=C, caption={Example PMP configuration for a NAPOT-aligned memory region.}, label={lst:pmp-napot}]
#define PMP_R 0x01
#define PMP_W 0x02
#define PMP_X 0x04
#define PMP_NAPOT 0x18

// Configure PMP entry 0 for a 4KB aligned enclave memory region
pmpaddr0 = (0x80000 >> 2);  // Shifted for NAPOT format
pmpcfg0  = PMP_R | PMP_W | PMP_X | PMP_NAPOT;
\end{lstlisting}

Ultimately, secure system designers must carefully consider the implications of these hardware and software limitations when deploying cryptographic workloads in secure enclave environments. The trade-offs between security, performance, and scalability will need to be continuously evaluated, particularly as cryptographic algorithms evolve and become more memory-intensive in response to the growing demands of secure communications and data privacy.

Moreover, cryptographic workloads, particularly those used in emerging fields like post-quantum cryptography, will require more efficient enclave architectures to ensure that they do not suffer from unacceptable performance degradation. As these workloads become increasingly critical in sectors such as secure cloud computing, IoT, and edge computing, addressing the bottlenecks created by the PMP mechanism will be essential to enabling the future scalability and practicality of secure enclaves.

\textbf{Note:} The above code snippets and figures are intended as conceptual illustrations to aid understanding. They represent typical scenarios and strategies but are not directly operational without further system integration and adaptation.

\section{Configuration Best Practices and Deployment Recommendations for RISC-V TEEs}
\label{sec:best-practices}
Deploying Trusted Execution Environments (TEEs) on RISC-V platforms using the Keystone framework presents a unique set of challenges and opportunities that must be carefully navigated to achieve optimal security and performance. Based on the extensive performance benchmarking and system analysis conducted in Chapters~\ref{chap:benchmarking} and \ref{chap:discussion}, several configuration best practices emerge, offering practical guidance for system architects, developers, and security engineers seeking to deploy enclaves across a variety of workload types and operational contexts.

A fundamental consideration when configuring RISC-V enclaves involves a holistic understanding of the interplay between hardware resources, isolation mechanisms, and workload characteristics. As discussed in Section~\ref{sec:synthesis} of Chapter~\ref{chap:discussion}, the Physical Memory Protection (PMP) mechanism stands at the heart of this relationship, providing the essential hardware-enforced boundary that isolates enclave memory from the untrusted environment. However, PMP’s inherent design—specifically the limited number of entries available and the strict Naturally Aligned Power-of-Two (NAPOT) encoding format for memory regions—imposes a critical scalability constraint that directly influences enclave instantiation and concurrency. Consequently, configuring enclave memory regions to maximize contiguity and alignment is paramount; doing so minimizes PMP entry consumption, reduces fragmentation, and alleviates potential bottlenecks in enclave creation and management. Developers are advised to meticulously design memory layouts that conform to PMP’s alignment constraints, thereby ensuring that enclave memory mappings are as compact and contiguous as possible (see Figure~\ref{fig:pmp-fragmentation-group}).

Memory capacity itself is another crucial parameter influencing enclave performance, as shown in the benchmarking results summarized in Section~\ref{sec:workload-sensitivity} (Chapter~\ref{chap:benchmarking}). Increasing the amount of memory allocated to enclaves generally results in improved throughput by reducing page faults and enhancing cache locality, which collectively diminishes the latency penalties introduced by enclave isolation. However, the law of diminishing returns applies here: beyond a certain threshold, particularly when memory regions become fragmented or non-contiguous, the incremental performance gains taper off, and the increased PMP entry overhead begins to outweigh benefits. Thus, system designers should balance memory allocation with PMP resource consumption carefully, favoring contiguous memory blocks that fit neatly within available PMP registers.

CPU core count also plays a significant role in performance scaling, especially for multi-threaded or parallelizable workloads (Section~\ref{sec:hardware-impact}, Chapter~\ref{chap:benchmarking}). While increasing the number of cores can yield meaningful performance improvements, our analysis indicates that these gains plateau beyond approximately four cores. This plateau arises from the overhead introduced by frequent context switches between host and enclave modes, as well as the synchronization costs required to maintain consistency across enclave threads. For compute-bound workloads such as CoreMark and Dhrystone, which exhibit predictable memory access patterns and limited memory intensity, this core scaling strategy remains effective within the noted limit. Conversely, memory-bound and latency-sensitive workloads like Kyber experience more pronounced performance degradation under enclave execution, and may require alternative approaches, such as workload-specific enclave optimizations or offloading of non-critical tasks to the untrusted environment.

The findings presented in Chapter~\ref{chap:discussion}—particularly the challenges introduced by the RISC-V PMP mechanism—underscore the complexity of deploying secure enclaves in real-world systems, especially when tasked with processing memory-intensive cryptographic workloads. The implications of these challenges for system designers are profound, as the trade-offs between security, performance, and scalability need to be carefully navigated. To this end, several recommendations can be made for optimizing Trusted Execution Environment (TEE) deployments, ensuring that future systems can overcome the current bottlenecks while maintaining strong isolation and security guarantees.

\begin{lstlisting}[caption={Best Practices for RISC-V TEE Deployment}, 
                   label={lst:enclave-best-practices}, 
                   basicstyle=\ttfamily\small, 
                   numbers=left, 
                   numberstyle=\tiny, 
                   xleftmargin=2em]
Allocate enclave memory contiguously, NAPOT-aligned, to minimize PMP usage.
Consolidate workloads into fewer enclaves to reduce resource contention.
Limit active enclaves to prevent PMP exhaustion and creation failures.
Offload non-sensitive tasks to the untrusted domain to lower overhead.
Batch enclave calls to amortize context-switch costs.
Use dynamic PMP management to optimize resources at runtime.
\end{lstlisting}

First and foremost, it is essential to adopt a workload-aware approach to enclave deployment, as detailed in Section~\ref{sec:implications} of Chapter~\ref{chap:discussion}. Enclaves should be selectively used for tasks that benefit most from strong memory isolation and security, such as sensitive key management or isolated data processing. For cryptographic workloads, especially those that are highly memory-intensive like post-quantum algorithms, a hybrid approach may be more appropriate. In such a configuration, sensitive key material can be securely stored within the enclave, while the bulk of the cryptographic operations, such as encryption and decryption, are performed outside the enclave. This approach minimizes the performance impact of running these tasks within an enclave, while still ensuring the confidentiality of the keys.

The performance challenges associated with memory-bound workloads, as highlighted by the benchmarking of Kyber and discussed in Chapter~\ref{chap:discussion}, suggest that hardware accelerators could play a pivotal role in improving enclave efficiency. Hardware-based acceleration for cryptographic algorithms—such as post-quantum schemes—could alleviate the overhead incurred by the enclave’s security enforcement. Incorporating specialized cryptographic co-processors within the TEE architecture would allow the bulk of cryptographic computations to be offloaded, reducing the load on the CPU and minimizing the need for frequent transitions between enclave and host contexts. This could significantly reduce the latency and overhead associated with cryptographic operations in secure environments.

Given the PMP entry limitations and the observed overheads of enclave context switches, a key deployment recommendation is to limit the number of simultaneously active enclaves to avoid PMP exhaustion, which can lead to explicit enclave creation failures (see Section~\ref{sec:workload-sensitivity}, Chapter~\ref{chap:benchmarking}). Practical configurations should, therefore, consolidate workloads into fewer, more capable enclaves when possible—a strategy known as enclave multiplexing. This approach not only conserves precious PMP entries but also reduces synchronization overhead and inter-enclave communication latency. Software-level techniques, including dynamic PMP entry management and runtime reconfiguration, can further optimize resource utilization by adapting PMP assignments based on workload activity and priority, though these require sophisticated coordination to maintain security invariants.

From a systems engineering perspective, the limitations of the PMP mechanism in its current form also warrant attention, as emphasized in Chapter~\ref{chap:discussion}. Expanding the number of available PMP entries or introducing more flexible memory region encoding schemes—such as support for larger or more fragmented memory regions—could alleviate some of the bottlenecks currently faced by multi-enclave deployments. These hardware modifications, however, would require changes to the underlying processor architecture and may not be feasible for immediate implementation. In the meantime, optimizing the use of existing PMP entries through dynamic allocation, memory layout optimization, and enclave multiplexing strategies offers a more immediate solution to the scalability challenges.

In terms of software strategies, significant improvements can be made by enhancing the runtime management of enclaves, as outlined in Section~\ref{sec:synthesis} (Chapter~\ref{chap:discussion}). For example, reducing the frequency of context switches between the enclave and host operating system, and batching enclave invocations, could help mitigate the performance degradation caused by these transitions. Additionally, improving the enclave lifecycle management to dynamically allocate resources based on the current workload could lead to more efficient use of limited resources, further enhancing scalability.

Moreover, careful attention should be given to the design of memory allocation and management strategies in multi-tenant environments, where multiple enclaves may need to coexist on the same system (Section~\ref{sec:implications}, Chapter~\ref{chap:benchmarking}). In such scenarios, strategies such as enclave multiplexing—where multiple tasks or applications are combined within a single enclave—could significantly reduce the overall resource requirements. This approach, while potentially reducing the level of isolation between tasks, could help manage the limited PMP entries more effectively, enabling a larger number of enclaves to coexist within the same system.

Finally, as the role of secure enclaves continues to evolve, it is critical that future TEEs are designed with scalability and flexibility in mind. As cryptographic workloads and the demand for secure execution environments grow, TEEs must be able to adapt to new threats and performance requirements. This means that future developments should not only focus on improving hardware capabilities but also on optimizing the software stack to make better use of available resources. Future research should also explore new isolation models that could reduce the dependency on rigid memory protection mechanisms like PMP, providing alternative approaches that can scale more effectively with increasing workload demands.

The successful deployment of RISC-V enclaves using Keystone hinges on a delicate balance of configuration parameters that directly affect performance, scalability, and security. By prioritizing contiguous and aligned memory allocations, judiciously scaling CPU resources, limiting enclave concurrency to avoid PMP exhaustion, and leveraging enclave multiplexing where appropriate, practitioners can tailor configurations to meet the demands of diverse workloads—ranging from compute-bound applications to cryptographic algorithms and multi-tenant edge scenarios. These best practices, grounded in empirical benchmarking and thorough architectural analysis from Chapters~\ref{chap:benchmarking} and \ref{chap:discussion}, serve as a foundational guide for maximizing the benefits of open-source TEEs on emerging RISC-V platforms, while illuminating pathways for future research and hardware innovation to further expand the design space of secure enclave execution.
